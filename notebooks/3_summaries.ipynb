{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e8ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import malnis_dataset\n",
    "from malnis_dataset import show\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2c23c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_id</th>\n",
       "      <th>text</th>\n",
       "      <th>query</th>\n",
       "      <th>reference_summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1</td>\n",
       "      <td>BERT fine-tuning; multi-document summarization...</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>For instance, pre-trained sentence embedding m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>643</td>\n",
       "      <td>BERT fine-tuning; multi-document summarization...</td>\n",
       "      <td>Transfer and multi-task learning have traditio...</td>\n",
       "      <td>[45] have presented a joint many-task model wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>233</td>\n",
       "      <td>BERT fine-tuning; multi-document summarization...</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>Recently, Liu and Lapata [43] have developed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>20</td>\n",
       "      <td>Text classification (Korde and Mahender, 2012;...</td>\n",
       "      <td>With the capability of modeling bidirectional ...</td>\n",
       "      <td>XLNet (Yang et al., 2019) learns bidirectional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>1751</td>\n",
       "      <td>Text classification (Korde and Mahender, 2012;...</td>\n",
       "      <td>Recently, pre-trained models have achieved sta...</td>\n",
       "      <td>0 (Sun et al., 2019) proposed a continual pret...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          reference_id                                               text  \\\n",
       "paper_id                                                                    \n",
       "204                  1  BERT fine-tuning; multi-document summarization...   \n",
       "204                643  BERT fine-tuning; multi-document summarization...   \n",
       "204                233  BERT fine-tuning; multi-document summarization...   \n",
       "855                 20  Text classification (Korde and Mahender, 2012;...   \n",
       "855               1751  Text classification (Korde and Mahender, 2012;...   \n",
       "\n",
       "                                                      query  \\\n",
       "paper_id                                                      \n",
       "204       We introduce a new language representation mod...   \n",
       "204       Transfer and multi-task learning have traditio...   \n",
       "204       Bidirectional Encoder Representations from Tra...   \n",
       "855       With the capability of modeling bidirectional ...   \n",
       "855       Recently, pre-trained models have achieved sta...   \n",
       "\n",
       "                                          reference_summary  \n",
       "paper_id                                                     \n",
       "204       For instance, pre-trained sentence embedding m...  \n",
       "204       [45] have presented a joint many-task model wi...  \n",
       "204       Recently, Liu and Lapata [43] have developed a...  \n",
       "855       XLNet (Yang et al., 2019) learns bidirectional...  \n",
       "855       0 (Sun et al., 2019) proposed a continual pret...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/clean_examples.csv\", index_col = 0)\n",
    "show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0a4be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paper_id\n",
       "204    BERT fine-tuning; multi-document summarization...\n",
       "204    BERT fine-tuning; multi-document summarization...\n",
       "204    BERT fine-tuning; multi-document summarization...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text[204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49240384",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87e17e495054385b6f7b9dd04096689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
      "\n",
      "['For instance, pre-trained sentence embedding models such as the Bidirectional Encoder Representations from Transformers (BERT) [19], Skip-Thoughts [20] and ELMo [21] have shown to be effective for learning universal sentences representations, which are useful for many natural language processing tasks including automatic text summarization methods [22].', 'We use a model-based transfer learning approach, where the pre-trained model BERT is applied to a specific task by adding additional few layers to the source model with parameters of some layers stay the same while parameters of other layers are learned using the new task-specific data.', 'In the second approach, a simple output layer is added to the pre-trained model, where all the parameters are jointly fine-tuned using task-specific data.', 'It plays a key role in many natural language processing tasks, more specifically in text representations learning.', '3.1.2.', 'The experimental results on benchmark datasets are presented in section 4 and discussed in section 5.', 'Following this success, other works have explored the potential of deep learning models for sentence representation learning.', 'The original architecture of BERT model is illustrated in Figure 1.', 'In this step, the pre-trained model BERT is fine-tuned for each GLUE task using taskspecific data.']\n",
      "\n",
      "max score: 0.4285714235887296\n",
      "******************************\n",
      "\n",
      "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.\n",
      "\n",
      "['[45] have presented a joint many-task model with growing depth in a single end-to-end model, which makes use of linguistic hierarchies to solve increasingly complex natural language processing tasks.', 'Transfer learning is considered as a promising strategy that allows leveraging knowledge learned from one or more related tasks to boost the performance of a target task.', 'We distinguish between two popular approaches for learning sentence embeddings, namely multi-task learning and language model pre-training.', 'Our idea is justified by the fact that transfer learning allows benefitting from knowledge learned from other natural language understanding tasks.']\n",
      "\n",
      "max score: 0.4311377195811969\n",
      "******************************\n",
      "\n",
      "Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings. Our code is available at https://github.com/nlpyang/PreSumm\n",
      "\n",
      "['For instance, pre-trained sentence embedding models such as the Bidirectional Encoder Representations from Transformers (BERT) [19], Skip-Thoughts [20] and ELMo [21] have shown to be effective for learning universal sentences representations, which are useful for many natural language processing tasks including automatic text summarization methods [22].', 'We investigate the impact of the two BERT fine-tuning methods on extractive multi-document summarization, and we showcase that BERT multi-task fine-tuning achieves substantial performance improvement.', '1–19 The Author(s), DOI: 10.1177/0165551521990616 have introduced a novel-based BERT document-level encoder able to capture the semantics of a document and thus generate representations of its sentences.', 'The latter can be explained by the fact that BERT model is trained on a masked language model where the output vectors are grounded to tokens instead of sentences, while sentence representation is the cornerstone of our unsupervised extractive multidocument summarization methods.', 'In this article, we propose an unsupervised extractive method for multi-document summarization based on transfer learning from the fine-tuned BERT models.', 'The obtained results show that the proposed method significantly outperforms several baseline methods and is on a par with recent state-of-the-art methods for extractive multi-document summarization.', '[14] have proposed a novel method for learning text representation across multiple tasks.', '1–19 The Author(s), DOI: 10.1177/0165551521990616 datasets.', 'Then, we provide a brief overview of the transfer learning methods applied in text representation learning.', 'Recently, Liu and Lapata [43] have developed a general framework for both extractive and abstractive text summarization based on the Bidirectional Encoder Representations from Transformers (BERT) [19], where authors Journal of Information Science, 2021, pp.', 'Since we introduce an unsupervised method for extractive multi-document summarization, we benefit from transfer learning abilities.']\n",
      "\n",
      "max score: 0.494545449635967\n",
      "******************************\n",
      "\n",
      "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.\n",
      "\n",
      "['XLNet (Yang et al., 2019) learns bidirectional contexts by maximizing expected likelihood over all permutations of factorization order and uses a generalized autoregressive pretraining mechanism to overcome the pretrain-finetune discrepancy of BERT.', 'In terms of best results, SSL-Reg (MTP) performs better than RoBERTa (our run) on 5 out of 9 datasets and achieves the same performance as RoBERTa (our run) on the rest 4 datasets.', '(2019) proposed a self-supervised representation learning approach based on maximizing mutual information between features extracted from multiple views of a shared context.', 'Texts with masked tokens are fed into a text encoder which learns a latent representation for each token including the masked ones.', 'From these tables, we make the following observations.', 'First, SSL-Reg outperforms unregularized RoBERTa significantly on all datasets.']\n",
      "\n",
      "max score: 0.40796019401103933\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recently, pre-trained models have achieved state-of-the-art results in various language understanding tasks, which indicates that pre-training on large-scale corpora may play a crucial role in natural language processing. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entity, semantic closeness and discourse relations. In order to extract to the fullest extent, the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which builds and learns incrementally pre-training tasks through constant multi-task learning. Experimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several common tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.\n",
      "\n",
      "['ERNIE 2.0 (Sun et al., 2019) proposed a continual pretraining framework which builds and learns incrementally pretraining tasks through constant multi-task learning, to capture lexical, syntactic and semantic information from training corpora.', 'These models have achieved substantial success in learning language representations.', 'Architecture of the classification model is the same as that in (Liu et al., 2019b).', 'In addition, we performed experiments on the datasets in the GLUE benchmark (Wang et al., 2018).', 'The other task is SSL.', 'Some words have only one Synset and some have several.', '• Unregularized BERT.', 'In NLP, various auxiliary tasks have been proposed for SSL, such as next token prediction in GPT (Radford et al.']\n",
      "\n",
      "max score: 0.471794866817094\n",
      "******************************\n",
      "\n",
      "Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, ie. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.\n",
      "\n",
      "['We have introduced a novel active learning method that is applicable to current deep networks with a wide range of tasks.', 'However, MSE is not a suitable choice for this problem since the scale of the real loss l changes (decreases in overall) as learning of the target model progresses.', 'For all tasks, we initialize a labeled dataset L0K by randomly sampling K=1,000 data points from the entire dataset UN .', 'Given a pool of unlabeled data, there have been three major approaches according to the selection criteria: an uncertainty-based approach, a diversity-based approach, and expected model change.', 'The intuition is that learning over a representative subset would be competitive over the whole pool.', 'However, constructing a committee is too expensive for current deep networks learned with large data.', 'The results show that our method outperforms other methods as the active learning cycle progresses.', 'The empirical analysis of [33, 20] suggests that the performance of recent deep networks is not yet saturated with respect to the size of training data.', 'These three tasks are indeed important research topics for visual recognition in computer vision, and are very useful for many real-world applications.', 'As an evaluation metric, we use the classification accuracy.', 'The selected data points would be more informative to the current model.', 'We use the training set as the initial unlabeled pool U22,246.', 'The simplest method of the uncertainty approach is to utilize class posterior probabilities to define uncertainty.', 'In this section, we introduce the proposed active learning method.', 'We choose this feature map to estimate the loss.']\n",
      "\n",
      "max score: 0.44827585715481577\n",
      "******************************\n",
      "\n",
      "For many types of machine learning algorithms, one can compute the statistically `optimal' way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.\n",
      "\n",
      "['While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate [74].', 'For many types of ML-algorithms, one can compute the statistically optimal way to select training data.', 'Here interactive machine learning (iML) may be of help, having its roots in reinforcement learning, preference learning, and active learning.']\n",
      "\n",
      "max score: 0.597014920463355\n",
      "******************************\n",
      "\n",
      "A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.\n",
      "\n",
      "['Baxter [80] introduced a model of bias learning which builds on the PAC learning model which concluded that learning multiple related tasks reduces the sampling burden required for good generalization and that the bias learnt on sufficiently many training tasks is likely to be good for learning novel tasks drawn from the same environment (the problem of transfer learning to new environments is discussed in the next subsection).', 'To find such a bias is still the hardest problem in any ML task and essential for the initial choice of an appropriate hypothesis space, which must be large enough to contain a solution, and small enough to ensure a good generalization from a small number of data sets.', 'However, such methods are limited by the accuracy and reliability of the experts knowledge (robustness of the human) and also by the extent to which that knowledge can be transferred to new tasks (see next subsection).', 'In a multi-task setting this requires the different learning tasks to share the same set of classes.', '6.2 Example: transfer learning (generalization) A huge problem in ML is the phenomenon of catastrophic forgetting, i.e., when having learned one task and being transferred to another task the ML-algorithm ‘‘forgets’’ how to perform the learned task.', 'Finally, they presented a local search technique for query optimization that worked well with large outcome spaces.', 'This is a much more difficult problem.']\n",
      "\n",
      "max score: 0.437735844213884\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.\n",
      "\n",
      "['The lottery tickets hypothesis of Frankle and Carbin [11] explores the possibility of pruning early in training by revealing that some sparse subnetworks inside neural networks can reach accuracy matching that of the full network when trained in isolation.', 'We show in this section that adding a little bit of label supervision for pruning improves the resulting winning tickets initializations performance on label classification, suggesting that these initializations are task-dependant to some extent.', 'They usually contain a huge number of parameters: some even count hundreds of millions of weights [3], [4] which is an order of magnitude bigger than standard networks used for supervised pre-training [5].', 'We evaluate the pruned subnetworks by training them from their initialization on semantic label classification.', 'we only have 100 classes).', 'We also find several limitations to our study.', 'Learning without supervision.']\n",
      "\n",
      "max score: 0.432900427950001\n",
      "******************************\n",
      "\n",
      "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.\n",
      "\n",
      "['Instead of formulating abstractive summarization as a seq2seq problem using an encoder-decoder architecture, we only use a single transformer language model that is trained from scratch, with appropriately “formatted” data (see figure 1, we also describe the formatting later in this section).', 'With our method, the resulting TLM can focus its attention on the relevant content and its model complexity on the summarization task.', 'Motivated by neural network success in machine translation experiments, the attention-based encoder decoder paradigm has recently been widely studied in abstractive summarization (Rush et al., 2015; Nallapati et al., 2016a; Chopra et al., 2016).', 'Such an approach could be thought of as a form of hard attention.', 'The attention weights at produce a context vector ct, which is then used to compute an attention aware hidden state h̃t.', 'In contrast, the extractive methods we use here are trained discriminatively using an extractive abstract as the target that is generated using an oracle.', 'It would be very interesting to see what kind of performance larger models could achieve.']\n",
      "\n",
      "max score: 0.4067796560776358\n",
      "******************************\n",
      "\n",
      "The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.\n",
      "\n",
      "['The key idea of AL is that a machine learning model can achieve a desired performance level using fewer training instances if it can select the data which is the most beneficial to its learning process [35].', '1) Acquisition Function: Given a model M , an unlabeled data pool Dl=0, a labeled data pool Dl=1 and observations x ∈ Dl=0, an AL algorithm uses an acquisition function a(x,M) to choose the next data sample(s) to be queried [12].', 'Settles (2009) [35] provides a comprehensive overview of the most commonly used query strategy frameworks.', '(2019) [21] improve the performance of the acquisition function BALD (Bayesian Active Learning by Diverse) [17] in the batch setting (BatchBALD).', 'Labels are obtained from an expert.', '(7) A CNN with these modifications is the basis for our proposed AL algorithm.']\n",
      "\n",
      "max score: 0.417910442764288\n",
      "******************************\n",
      "\n",
      "The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.\n",
      "\n",
      "['It’s well known that the key idea lying behind active learning is a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose data from which it learns [13].', 'Active Learning is a setup that allows the learning algorithm to iteratively and strategically query the labels of some instances for reducing human labeling efforts.', 'What’s worse, obtaining more labeled instances is expensive and even unrealistic in practical applications.', 'In many real-word applications, the labels are difficult to obtain because of the scarce of experts or the lack of money.', 'In order to deal with these issues, Active Learning is proposed.', 'Another general active learning framework is QBC.']\n",
      "\n",
      "max score: 0.49484535584387296\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.\n",
      "\n",
      "['The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns [1].', 'This allows to compute the uncertainty in the form of entropy for each bounding box.', 'Then, selected images are labeled and added to the training set.', 'Second, performance: by involving the model in building the training dataset, it may be possible to achieve higher performance with fewer data samples, e.g., the model may choose images it learns most from– something often difficult for a human.', 'More precisely, the algorithm finds a sparse linear combination to represent the uncertainty of unlabeled data where diversity is also incorporated.']\n",
      "\n",
      "max score: 0.4897959133803624\n",
      "******************************\n",
      "\n",
      "The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.\n",
      "\n",
      "['For example, DUALIST [78] is an active learning tool that queries annotators for labels of both instances (e.g., whether a text document is about “baseball” or “hockey”) and features (which keywords, if appeared in a document, are likely indicators that the document is about “baseball”).', 'This idea resonates with the critical challenge in modern ML, that labeled data are time-consuming and expensive to obtain [102].', 'Algorithmic work of AL assumes the human annotator to be an oracle that provides error-free labels [77], while in reality annotation errors are commonplace and can be systematically biased by a particular AL setting.', 'The core idea of AL is that if a learning algorithm intelligently selects instances to be labeled, it could perform well with much less training data [77].', 'One promising learning paradigm is Active Learning (AL), by which the model intelligently selects instances to query a machine teacher for labels, so that the labeling workload could be largely reduced.']\n",
      "\n",
      "max score: 0.4150943346266465\n",
      "******************************\n",
      "\n",
      "Attention mechanism has enhanced state-of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT.\n",
      "\n",
      "['3) Coverage [12]: The coverage model was first proposed for the NMT task [105] to address the problems of the standard attention mechanism which tends to ignore the past alignment information.', 'It has achieved state-of-the-art performance in machine translation task with significantly less training time.', 'In this paper, we only consider the single-layer forward LSTM13 for both word and chunk encoders.', '11 1) Hierarchical Attention [14]: The intuition behind a hierarchical attention is that words in less important chunks should be less attended.', 'To alleviate this problem, Ranzato et al.', 'Inspired by the success of neural machine translation (NMT) [23], Rush et al.', '[38] introduced a read-again mechanism to improve the quality of the representations of the source texts.']\n",
      "\n",
      "max score: 0.45882352444567476\n",
      "******************************\n",
      "\n",
      "We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.\n",
      "\n",
      "['[15] first introduced a neural attention seq2seq model with an attention based encoder and a neural network language model (NNLM) decoder to the abstractive sentence summarization task, which has achieved a significant performance improvement over conventional methods.', 'This model can control the information flow from the encoder to the decoder via constructing a second level representation of the source texts with the gate network.', '1) Selective Encoding [74]: The selective encoding model was proposed for the abstractive sentence summarization task [74].', 'It also achieved state-of-the-art performance on several machine translation benchmark datasets.', 'Our experimental results are shown in Table VIII.', 'A vanilla seq2seq framework for the abstractive summarization is composed of an encoder and a decoder.', 'We implemented the attention based seq2seq model shown in Fig.', 'Seq2seq models (see Fig.']\n",
      "\n",
      "max score: 0.5384615335281066\n",
      "******************************\n",
      "\n",
      "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.\n",
      "\n",
      "['Most of the prevalent seq2seq models that have attained state-of-the-art performance for sequence modeling and language generation tasks are RNN, especially long short-term memory (LSTM) [32] and gated recurrent unit (GRU) [60], 1https://github.com/yaserkl/RLSeq2Seq/ 3 based encoder-decoder models [19, 23].', 'We also conducted extensive experiments on this dataset using NATS to examine the effectiveness of different neural network components.', 'Then, we will describe more advanced network structures that can handle different challenges in the text summarization, such as repetition and out-of-vocabulary (OOV) words.', 'Encoder and decoder can be chosen to be either LSTM or GRU.', '[76] proposed a deliberation network for sequence generation tasks.', 'It can be observed that C10110 performs better than G11110.', 'In the encoder-decoder framework depicted in Fig.']\n",
      "\n",
      "max score: 0.44999999522578127\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-B candidates - resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space - implying that DBS is a better search algorithm. Moreover, these gains are achieved with minimal computational or memory over- head as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Further, we study the role of diversity for image-grounded language generation tasks as the complexity of the image changes. We observe that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.\n",
      "\n",
      "['2) Diverse Beam Search (DBS) [94, 95]: DBS is another approach that aims to increase the diversity of standard beam search algorithm.', 'The proposed ByteNet model has achieved state-of-the-art performance on a character-level machine translation task with parallelism and linear-time computational complexity [65].', '[94] proposed generating diverse outputs by optimizing for a diversity-augmented objective function.', 'RNN encoder-decoder models have been commonly used in sequence modeling and language generation tasks.', 'In other words, the top-B hypotheses may differ by just a couple tokens at the end of sequences, which not only affects the quality of generated sequences but also wastes computational resources [90, 94].', 'Their method, referred to as Diverse Beam Search (DBS) algorithm, has been applied to image captioning, machine translation, and visual question-generation tasks.', 'To alleviate this problem, a RL-based method has been proposed to find the best diversity rate γ with respect to the given evaluation metrics.', 'Thus, we did not report these results.', 'Texts are tokenized using Stanford CoreNLP package and prepared with our data processing tool28.', 'Beam search is a graph-search algorithm that generates sequences from left to right by retaining only B top scoring (top-B) sequence-fragments at each decoding step.', 'Here, the vocabulary is viewed as an action space.', 'This algorithm has shown a better performance for greedy generation compared to XENT, DAD and E2E in the task of abstractive text summarization.', 'This is the first method to approximate this objective.', 'In other words, top-K candidates are nearly identical, where K is size of a beam.', 'We will highlight various existing problems and proposed solutions.', 'Here, V represents the output vocabulary.', 'In this section, we review different encoder-decoder models for the neural abstractive text summarization.']\n",
      "\n",
      "max score: 0.47619047126860126\n",
      "******************************\n",
      "\n",
      "In this paper, we propose a simple, fast decoding algorithm that fosters diversity in neural generation. The algorithm modifies the standard beam search algorithm by adding an inter-sibling ranking penalty, favoring choosing hypotheses from diverse parents. We evaluate the proposed model on the tasks of dialogue response generation, abstractive summarization and machine translation. We find that diverse decoding helps across all tasks, especially those for which reranking is needed. We further propose a variation that is capable of automatically adjusting its diversity decoding rates for different inputs using reinforcement learning (RL). We observe a further performance boost from this RL technique. This paper includes material from the unpublished script \"Mutual Information and Diverse Decoding Improve Neural Machine Translation\" (Li and Jurafsky, 2016).\n",
      "\n",
      "['In [71], the authors proposed a convolutional encoder model, in which the encoder is composed of a succession of convolutional layers, and demonstrated its strong performance for machine translation.', 'In this section, we briefly introduce some studies that aim to improve decoding by increasing the diversity of the beam search algorithm.', 'They further proposed a novel framework [87] that is composed of two auxiliary tasks, i.e., question generation and entailment generation, to improve their model for capturing the saliency and entailment for the abstractive text summarization.', 'We also conducted extensive experiments on this dataset using NATS to examine the effectiveness of different neural network components.', 'These problems are alleviated by the curriculum learning and reinforcement learning (RL) approaches.', 'This is a two-step procedure.', 'As to intradecoder attention, we observe from G11000, G11010, G11100 and G11110 that it does not boost the performance of the model before adding weight sharing mechanism.', '2: The basic seq2seq model.', 'A vanilla seq2seq framework for the abstractive summarization is composed of an encoder and a decoder.', '[76] proposed a deliberation network for sequence generation tasks.']\n",
      "\n",
      "max score: 0.42201834369497515\n",
      "******************************\n",
      "\n",
      "We present a training framework for neural abstractive summarization based on actor-critic approaches from reinforcement learning. In the traditional neural network based methods, the objective is only to maximize the likelihood of the predicted summaries, no other assessment constraints are considered, which may generate low-quality summaries or even incorrect sentences. To alleviate this problem, we employ an actor-critic framework to enhance the training procedure. For the actor, we employ the typical attention based sequence-to-sequence (seq2seq) framework as the policy network for summary generation. For the critic, we combine the maximum likelihood estimator with a well designed global summary quality estimator which is a neural network based binary classifier aiming to make the generated summaries indistinguishable from the human-written ones. Policy gradient method is used to conduct the parameter learning. An alternating training strategy is proposed to conduct the joint training of the actor and critic models. Extensive experiments on some benchmark datasets in different languages show that our framework achieves improvements over the state-of-the-art methods.\n",
      "\n",
      "['[58] proposed a training framework based on the actor-critic method, where the actor network is an attention-based seq2seq model, and the critic network consists of a maximum likelihood estimator and a global summary quality estimator that is used to distinguish the generated and ground-truth summaries via a neural network binary classifier.', 'In this survey, we created six datasets with different tokenizers, three of which are for text summarization and the rest of them are used for headline generation.', 'The rest of this paper is organized as follows: An overall taxonomy of topics on seq2seq models for neural abstractive text summarization is shown in Fig.', 'To alleviate this problem, Ranzato et al.', 'We implemented the attention based seq2seq model shown in Fig.', 'This is the first method to approximate this objective.', 'These problems are alleviated by the curriculum learning and reinforcement learning (RL) approaches.', 'This is a two-step procedure.']\n",
      "\n",
      "max score: 0.5308056822227712\n",
      "******************************\n",
      "\n",
      "We introduce The Benchmark of Linguistic Minimal Pairs (shortened to BLiMP), a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars, and aggregate human agreement with the labels is 96.4%. We use it to evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs. We find that state-of-the-art models identify morphological contrasts reliably, but they struggle with semantic restrictions on the distribution of quantifiers and negative polarity items and subtle syntactic phenomena such as extraction islands.\n",
      "\n",
      "['This example is from the Benchmark of Linguistic Minimal Pairs (BLiMP) (Warstadt et al., 2020), a challenge set of 67k pairs which isolate contrasts in syntax, morphology, and semantics (in this example, determiner-noun agreement).', 'We use PLLs to perform unsupervised acceptability judgments on the BLiMP minimal pairs set (Warstadt et al., 2020); BERT and RoBERTa models improve the state of the art (GPT-2 probabilities) by up to 3.9% absolute, with +10% on island effects and NPI licensing phenomena.', 'RoBERTa improves by around 10% on filler-gap dependencies, island effects, and negative polarity items (NPIs), largely closing the human gap.', 'For example, one might have no pretrained multilingual LMs for decoder initialization or fusion, as such models are difficult to train (Ragni et al., 2016).', 'We then evaluate on the test set.', 'Discriminative language modeling.']\n",
      "\n",
      "max score: 0.3842364482554782\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.\n",
      "\n",
      "['This method often substantially outperforms training on the supervised datasets from scratch, and a single pretrained language model often can be fine-tuned for state of the art performance on many different supervised datasets (Howard and Ruder, 2018).', 'From the novelty stats, we see that our RL fine-tuning consistently causes models to copy more.', 'We conclude with a few lessons and directions we plan to consider in future reward learning work.', 'Our results are mixed.', '(2017) used RL on BLEU but applied several error models to approximate human behavior.', 'Batched data collection is also a well-studied setting for active learning techniques.']\n",
      "\n",
      "max score: 0.4146341413414634\n",
      "******************************\n",
      "\n",
      "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.\n",
      "\n",
      "['Howard and Ruder [41] developed Universal Language Model Fine-tuning (ULMFiT), a LM transfer learning method using the AWD-LSTM architecture [64], which outperformed the state of the art on several text classification datasets when trained on only 100 labeled examples, and thereby achieved results significantly superior to more sophisticated architectures of previous work.', 'Another favorable development is that transfer learning, especially the paradigm of fine-tuning pre-trained language models (LMs), has become popular in NLP.', 'Nonetheless, it is to be investigated how little data is still necessary to successfully fine-tune a model.', 'they are usually not that effective for NLP tasks.', 'Nevertheless, correlation is a factor orthogonal to our taxonomy and can be added as an additional criterion.', 'We survey existing work at the intersection of AL, text classification, and (D)NNs.', 'Small Data DNNs DL approaches are usually applied in the context of large datasets.']\n",
      "\n",
      "max score: 0.4623115529436126\n",
      "******************************\n",
      "\n",
      "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.\n",
      "\n",
      "['Although these two algorithms are based on networks, the bipartite network is a direct mapping from vector space model representations.', '[34] to learn continuous vector representations of words from very large datasets.', 'The execution of the experimental configurations previously described resulted in 104 classification performance results for each tested document representation and dataset.', 'In particular, we propose two document collection 1 https://code.google.com/archive/p/word2vec/.', 'These word embeddings and NASARI vectors share the same semantic vector space, a property that is exploited in this work.', 'The fixed number of dimensions of the embeddings is usually lower than the number of dimensions of a bag-of-words.', 'The semantic representations compared in this experimental evaluation have a fixed dimensionality.', 'Commonly, machine learning algorithms are used to construct a general classification model based on previously labeled documents, i.e., training data.']\n",
      "\n",
      "max score: 0.44827585711124324\n",
      "******************************\n",
      "\n",
      "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.\n",
      "\n",
      "['The training objective of the Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document.', 'The performance of various Skip-gram models on the word analogy test set is reported in Table 1.', 'We show that subsampling of frequent words during training results in a significant speedup (around 2x - 10x), and improves accuracy of the representations of less frequent words.', 'Surprisingly, while we found the Hierarchical Softmax to achieve lower performance when trained without subsampling, it became the best performing method when we downsampled the frequent words.', '[8] introduced the Skip-gram model, an efficient method for learning highquality vector representations of words from large amounts of unstructured text data.', 'The word representations computed using neural networks are very interesting because the learned vectors explicitly encode many linguistic regularities and patterns.', 'Such words usually provide less information value than the rare words.', 'The techniques introduced in this paper can be used also for training the continuous bag-of-words model introduced in [8].']\n",
      "\n",
      "max score: 0.43749999515679255\n",
      "******************************\n",
      "\n",
      "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.\n",
      "\n",
      "['In 2011, a unified deep learning-based architecture for NLP was introduced, which is able to solve different NLP tasks such as name entity recognition, part of speech tagging, semantic role labeling, and chunking [10,11], the architecture avoids task-specific engineering as much as possible and rely on great amount of unlabeled data sets to discover internal representations which are applicable for all mentioned tasks.', 'This kind of network mostly is used to feature learning by encoding inputs data.', 'As a result, there do not exist any method which can be appropriately compared with our work.', 'This system shows how important each word is.', 'Table 1 shows a combination of these features to form five cases of the feature vector for training the deep neural network.', 'Since a long time ago, summarization has been considered by natural language processing researchers.']\n",
      "\n",
      "max score: 0.43999999514450006\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.\n",
      "\n",
      "['Rei [123] proposed a framework with a secondary objective – learning to predict surrounding words for each word in the dataset.', 'The added language modeling objective encourages the system to learn richer feature representations which are then reused for sequence labeling.', 'We also list a number of domain specific datasets, particularly developed on PubMed and MEDLINE texts.', 'For instance, a same named entity may be annotated with different types.', 'This multi-task mechanism lets the training algorithm to discover internal representations that are useful for all the tasks of interest.']\n",
      "\n",
      "max score: 0.5135135085135135\n",
      "******************************\n",
      "\n",
      "We present a deep hierarchical recurrent neural network for sequence tagging. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels to encode morphology and context information, and applies a conditional random field layer to predict the tags. Our model is task independent, language independent, and feature engineering free. We further extend our model to multi-task and cross-lingual joint training by sharing the architecture and parameters. Our model achieves state-of-the-art results in multiple languages on several benchmark tasks including POS tagging, chunking, and NER. We also demonstrate that multi-task and cross-lingual joint training can improve the performance in various cases.\n",
      "\n",
      "['They further extended their model to cross-lingual and multi-task joint trained by sharing the architecture and parameters.', '[105] employed deep GRUs on both character and word levels to encode morphology and context information.', 'Second, these language model embeddings can be further fine-tuned with one additional output layer for a wide range of tasks including NER and chunking.', 'A conditional random field (CRF) is a random field globally conditioned on the observation sequence [72].', 'Tag decoder predict tags for tokens in the input sequence.', 'They utilized the hidden states of a forwardbackward recurrent neural network to create contextualized word embeddings.', 'We then present a comprehensive survey on deep learning techniques for NER.', 'Experimental results demonstrate that multi-task learning is an effective approach to guide the language model to learn task-specific knowledge.']\n",
      "\n",
      "max score: 0.5317919025627319\n",
      "******************************\n",
      "\n",
      "We investigate a lattice-structured LSTM model for Chinese NER, which encodes a sequence of input characters as well as all potential words that match a lexicon. Compared with character-based methods, our model explicitly leverages word and word sequence information. Compared with word-based methods, lattice LSTM does not suffer from segmentation errors. Gated recurrent cells allow our model to choose the most relevant characters and words from a sentence for better NER results. Experiments on various datasets show that lattice LSTM outperforms both word-based and character-based LSTM baselines, achieving the best results.\n",
      "\n",
      "['Zhang and Yang [149] proposed a lattice-structured LSTM model for Chinese NER, which encodes a sequence of input characters as well as all potential words that match a lexicon.', 'the input to NER.', 'representation from the character sequence of a word.', 'Figure 9 illustrates the architecture with a short sentence on the NER task.', 'We then introduce the widely-used NER datasets and tools.']\n",
      "\n",
      "max score: 0.5666666616888889\n",
      "******************************\n",
      "\n",
      "In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and its context, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.\n",
      "\n",
      "['In this work, we propose debiased contrastive learning, a new unsupervised contrastive representation learning framework that corrects for the bias introduced by the common practice of sampling negative (dissimilar) examples for a point from the overall data distribution.', 'For language, Logeswaran and Lee [28] treat the context sentences as positive samples to efficiently learn sentence representations.', '(11) We begin by showing that the asymptotic unbiased contrastive loss is an upper bound on the supervised loss of the mean classifier.', 'In this work, we demonstrate that this is indeed possible, while still assuming only access to unlabeled training data and positive examples.', 'We retrain each model 3 times and show the average in Table 2.', 'The proposed framework is accompanied by generalization guarantees for the downstream classification task.', 'Acknowledgements This work was supported by MIT-IBM Watson AI Lab.']\n",
      "\n",
      "max score: 0.426395934180216\n",
      "******************************\n",
      "\n",
      "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.\n",
      "\n",
      "['GCN [16] models graph-structured data based on an efficient variant of convolutional neural networks, which has been proved effective in many NLP tasks.', 'In our experiments, the BASE architecture with a 768 hidden sizes 12-layer Transformer and 12 attention heads is used for both BERT and RoBERTa.', 'GAT [35] enables specifying different weights to different nodes and outperforms its convolutional counterparts on knowledge graph dataset.', 'In this section, we present our document information extraction model architecture and pre-trained methods.', 'Node representations of the last GCN layer are used as layout features for entity extraction.', 'We use GCN to encode various rich layout information and transformer-based pre-trained language models to encode text information.', 'Our model combines text features and layout features to perform entity extraction.']\n",
      "\n",
      "max score: 0.46783625251940775\n",
      "******************************\n",
      "\n",
      "We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation, we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a segmentation mask and bounding boxes. We demonstrate its capabilities on an information extraction task from invoices and show that it significantly outperforms approaches based on sequential text or document images.\n",
      "\n",
      "['Chargrid [14] models the problem by encoding each document page as a two-dimensional grid of characters and used a fully convolutional encoder-decoder network to predict the class of all the characters and group them by the predicted item bounding boxes.', 'Experimental results on two datasets and on the few-shot setting suggest that incorporating rich layout information and expressive text representation significantly improves extraction performance and reduces annotation cost for information extraction from visually rich documents.', 'The third category is approaches that exploit 2D grid information of characters or words.', 'We also use the fine-tuned weights of MLM task to initialize the SPRC task.', 'In this section, we present our document information extraction model architecture and pre-trained methods.']\n",
      "\n",
      "max score: 0.516129027349844\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.\n",
      "\n",
      "['Lin [36] introduced a set ofmetrics called RecallOriented Understudy for Gisting Evaluation (ROUGE) to automatically determine the quality of a summary by comparing it to human (reference) summaries.', '[56] used a machine learning technique and included features related to the thread as well as features of the email structure such as position of the sentence in the tread, number of recipients, etc.', 'In more recent paradigms, in particular TAC, query-based summaries have been created.', 'The second approach measures the density of the topic words.', 'ROUGE is the most widely used metric for automatic evaluation.', 'It is difficult for humans to summarize large amounts of text.', 'Graph-based methods can be used for single as well as multidocument summarization [24].', 'This kind of method ranks sentences by computing their salience using a set of features.']\n",
      "\n",
      "max score: 0.4252873514513146\n",
      "******************************\n",
      "\n",
      "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.\n",
      "\n",
      "['Recall-Oriented Understudy for Gisting Evaluation (ROUGE) is a set of metrics used to automatically evaluate summarization systems [12] by measuring the overlap between computer-generated summaries and multiple human written reference summaries.', 'The evaluation scripts have been provided at the CL-SciSumm Github repository9 where the participants may run their own evaluation and report the results.', 'This paper provides the results for the CL-SciSumm 2018 Task being held as part of the BIRNDL 2018 workshop at SIGIR 2018 in Ann Arbor, Michigan.', 'ROUGE–2 measures the bigram overlap between the candidate computergenerated summary and the reference summaries.', 'Summaries of the RP were also included.']\n",
      "\n",
      "max score: 0.38993710195166337\n",
      "******************************\n",
      "\n",
      "Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly outperforms state-of-the-art models.\n",
      "\n",
      "['While most current neural abstractive summarization models have focused on summarizing relatively short news articles (e.g., (See et al., 2017)), few researchers have started to investigate the summarization of longer documents by exploiting their natural structure.', 'One exception is (Cohan et al., 2018) that recently introduce two large-scale datasets of long and structured scientific papers obtained from arXiv and PubMed.', '(2018) also propose a model for abstractive summarization taking the structure of documents into consideration with a hierarchical approach, and test it on longer documents with section information, i.e.', '(ii) We test our method on the Pubmed and arXiv datasets and results appear to support our goal of effectively summarizing long documents.', 'Our model comprises three components: the sentence encoder, the document encoder and the sentence classifier.']\n",
      "\n",
      "max score: 0.43708608798386045\n",
      "******************************\n",
      "\n",
      "In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.\n",
      "\n",
      "['For this literature review, we have chosen the term neural (rather than deep) because: (1) most current work on textual NNs in NLP and IR is often actually quite shallow in the number of layers used (typically only a few hidden layers and often only one, though some notable recent exceptions exist, such as Conneau et al.', 'In terms of textual similarity, they argue that the ad-hoc retrieval task is mainly about relevance matching, different from semantic matching in NLP.', 'Experimental results show improvements by the integrated model over both standalone deep neural networks for query classification and a standalone DSSM for web search ranking.', 'puter vision, and natural language processing.', 'Question answering: This class includes tasks that are focused on retrieval of text segments that answers the user question.', 'By using NCE, the probability density estimation problem is converted to a binary classification problem.', '(2016a)’s results indicate that full-text search with DSSM does not perform as well as traditional IR models.', 'They then feed the histogram into a feed forward neural network.', 'They articulate three key differences they perceive between semantic and relevance matching: 1.', 'As mentioned in our discussion of Eq.', '(2016b) propose an attention-based neural matching model (aNMM) for question answering.', 'The most well-known and most widely used context-predicting models, word2vec (Mikolov et al.', '(2014) classify existing DSMs into two categories: context-counting and context-predicting.', 'However, these choices are not justified, and their later work Grbovic et al.', '2014), have been used extensively in recent work on web search.', 'The architecture of this model is depicted in Fig.', 'Experimental results show that the newly proposed models achieve state-of-the-art results.', 'This is the first work that employs an attention module for a web search task.']\n",
      "\n",
      "max score: 0.47826086468577605\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.\n",
      "\n",
      "['Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017).', 'We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis.', 'Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performance over the baseline.', 'We use vectors derived from a bidirectional LSTM that is trained with a coupled lan- guage model (LM) objective on a large text corpus.', 'In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.', 'However, these approaches for learning word vectors only allow a single contextindependent representation for each word.', '3.4) and easily incorporated into a wide range of existing neural NLP architectures (Sec.']\n",
      "\n",
      "max score: 0.4541484668263382\n",
      "******************************\n",
      "\n",
      "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.\n",
      "\n",
      "['Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017).', 'We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis.', 'Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performance over the baseline.', 'We use vectors derived from a bidirectional LSTM that is trained with a coupled lan- guage model (LM) objective on a large text corpus.', 'In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.', 'The pretrained GloVe vectors are fine-tuned during training.', 'However, these approaches for learning word vectors only allow a single contextindependent representation for each word.', '3.4) and easily incorporated into a wide range of existing neural NLP architectures (Sec.']\n",
      "\n",
      "max score: 0.4595744633394296\n",
      "******************************\n",
      "\n",
      "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.\n",
      "\n",
      "['Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017).', 'We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis.', 'Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performance over the baseline.', 'We use vectors derived from a bidirectional LSTM that is trained with a coupled lan- guage model (LM) objective on a large text corpus.', 'In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.', 'The pretrained GloVe vectors are fine-tuned during training.', 'However, these approaches for learning word vectors only allow a single contextindependent representation for each word.', '3.4) and easily incorporated into a wide range of existing neural NLP architectures (Sec.']\n",
      "\n",
      "max score: 0.4595744633394296\n",
      "******************************\n",
      "\n",
      "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at https://github.com/qizhex/RACE_AR_baselines.\n",
      "\n",
      "['The RACE dataset [17] contains near 100K questions taken from the English exams for middle and high school Chinese students in the age range between 12 to 18, with the answers generated by human experts.', 'Moreover, the average length of the passages in RACE are longer than 300, which is significantly longer than other popular reading comprehension datasets such as SQuAD [26].', '1Pretrained models and code are available at https://github.com/zihangdai/xlnet Preprint.', 'As a result, this dataset serves as a challenging benchmark for long text understanding.', 'This is one of the most difficult reading comprehension datasets that involve challenging reasoning questions.']\n",
      "\n",
      "max score: 0.550561792803939\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.\n",
      "\n",
      "['We compare with several strong textual models: SIF (Arora et al., 2017), a method for learning document representations by removing the first principal component of aggregated word-level embeddings which we pretrain on scientific text; SciBERT (Beltagy et al., 2019) a state-of-the-art pretrained Transformer LM for scientific text; and Sent-BERT (Reimers and Gurevych, 2019), a model that uses negative sampling to tune BERT for producing optimal sentence embeddings.', 'We emphasize that our citation-based pretraining objective is critical for the performance of SPECTER; removing this and using a vanilla SciBERT results in decreased performance on all tasks.', 'We achieve substantial improvements over the strongest of a wide variety of baselines, demonstrating the effectiveness of our model.', 'We release our code and data to facilitate reproducibility.', 'The landscape of Transformer language models is rapidly changing and newer and larger models are frequently introduced.', 'Specifically, we use SciBERT (Beltagy et al., 2019) which is an adaptation of the original BERT (Devlin et al., 2019) architecture to the scientific domain.', 'We also remove incoming citations from development and test set queries as these would not be available at test time in production.']\n",
      "\n",
      "max score: 0.4150943349234604\n",
      "******************************\n",
      "\n",
      "Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classification. In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.\n",
      "\n",
      "['In [48, 49], instead of using pre-trained low-dimensional word vectors as input to CNNs, the authors directly apply CNNs to high-dimensional text data to learn the embeddings of small text regions for classification.', 'What is a minimal neural network architecture that can achieve a certain accuracy on a given dataset?', 'Word-level embedding models suffer from problems such as OOV.', 'The Recurrent CNN [134] applies a recurrent structure to capture long-range contextual dependence for learning word representations.', 'Transformers, which will be described later, also use self-attention.', 'CapsNet-B performs better in the experiments.', 'This paper only discusses extractive QA.', '(2) Domain adaptation.', 'However, a simple shallow-and-wide network outperforms deep models such as DenseNet[58] with word inputs.', 'As we can see from Fig.']\n",
      "\n",
      "max score: 0.47619047120181407\n",
      "******************************\n",
      "\n",
      "We introduce VAMPIRE, a lightweight pretraining framework for effective text classification when data and computing resources are limited. We pretrain a unigram document model as a variational autoencoder on in-domain, unlabeled data and use its internal states as features in a downstream classifier. Empirically, we show the relative strength of VAMPIRE against computationally expensive contextual embeddings and other popular semi-supervised baselines under low resource settings. We also find that fine-tuning to in-domain data is crucial to achieving decent performance from contextual embeddings when working with limited supervision. We accompany this paper with code to pretrain and use VAMPIRE embeddings in downstream tasks.\n",
      "\n",
      "['[168] pre-train a document model as a VAE on in-domain, unlabeled data and use its internal states as features for text classification.', 'MRR is often used to evaluate the performance of ranking algorithms in NLP tasks such as query-document ranking and QA.', 'PLMs are expensive to serve.', 'Experiments show that multi-head attention is more effective than single-head attention.', 'We also provide a summary of more than 40 popular datasets widely used for text classification.', 'As we can see from Fig.', '(4) Task-specific fine-tuning.', 'These works, although important, are beyond the scope of this paper.', '[41] propose a bilateral multi-perspective matching model under the “matching-aggregation” framework.', 'Abstracting with credit is permitted.', 'There are other interesting attention models.']\n",
      "\n",
      "max score: 0.48809523312996034\n",
      "******************************\n",
      "\n",
      "We study in this work the importance of depth in convolutional models for text classification, either when character or word inputs are considered. We show on 5 standard text classification and sentiment analysis tasks that deep models indeed give better performances than shallow networks when the text input is represented as a sequence of characters. However, a simple shallow-and-wide network outperforms deep models such as DenseNet with word inputs. Our shallow word model further establishes new state-of-the-art performances on two datasets: Yelp Binary (95.9\\%) and Yelp Full (64.9\\%).\n",
      "\n",
      "['[57] show that deep models indeed outperform shallow models when the text input is represented as a sequence of characters.', 'However, a simple shallow-and-wide network outperforms deep models such as DenseNet[58] with word inputs.', 'Yelp [185] dataset contains the data for two sentiment classification tasks.', 'Various versions of this dataset are used for text classification, text clustering and so one.', '5.', 'CapsNet-B performs better in the experiments.']\n",
      "\n",
      "max score: 0.6249999950305177\n",
      "******************************\n",
      "\n",
      "Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the \\textbf{LayoutLM} to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at \\url{https://aka.ms/layoutlm}.\n",
      "\n",
      "['(Xu et al., 2020) proposed the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents.', 'In recent years, pre-training techniques have become more and more popular in both NLP and CV areas, and have also been leveraged in the VrDU tasks.', 'Experiment results show that the LayoutLMv2 model outperforms strong baselines including the vanilla LayoutLM and achieves new state-of-the-art results in these downstream VrDU tasks, which substantially benefits a great number of real-world document understanding tasks.', 'The pre-trained LayoutLMv2 model is publicly available at https://aka.ms/layoutlmv2.', 'The FUNSD dataset is suitable for a variety of tasks, where we focus on semantic entity labeling in this paper.', 'We first evaluate the effect of introducing visual information.', 'We use the publicly available PyTorch models for BERT (Wolf et al., 2020) and LayoutLM,4 and use our in-house implementation for the UniLMv2 models.']\n",
      "\n",
      "max score: 0.5596707768971533\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).\n",
      "\n",
      "['Prediction difference analysis is a method that highlights features in an image to provide evidence for or against a certain class [66].', 'When in the deep learning process is visualization used and best suited?', 'Their knowledge expedites key decisions in deep learning workflows, such as identifying the which types of models perform best on which types of data.', 'This mapping is one of the characteristics that allows a neural network to learn.']\n",
      "\n",
      "max score: 0.470588230449827\n",
      "******************************\n",
      "\n",
      "There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.\n",
      "\n",
      "['Much of Miller’s work highlights vast and valuable bodies of research in philosophy, psychology, and cognitive science for how people define, generate, select, evaluate, and present explanations, and he argues that interpretability and explainability research should leverage and build upon this history [62].', 'He suggests that if the focus on explaining decisions or actions to a human observer is the goal, then if these techniques are to succeed, the explanations they generate should have a structure that humans accept.', 'This section describes when visualizing deep learning may be most relevant and useful.', 'However, some systems suffer from scalability problems.', 'This division provides a concise summary for practitioners who wish to investigate the usage of the described techniques for their own work, and provides new researchers with the main venues for this research area to investigate existing literature.', 'Where has deep learning visualization been used?', 'During inference, we can recover the activations produced at each layer.']\n",
      "\n",
      "max score: 0.4904214509922051\n",
      "******************************\n",
      "\n",
      "We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.\n",
      "\n",
      "['(2017), for example, learn an embedding function that is used to represent the input instances such that simple nearest-prototype classifiers can be trained effectively to recognise instances of a new target class after being exposed to a small number of labeled examples.', 'With this approach, they achieved state-of-the-art results on various NLP tasks, such as Named Entity Recognition (token-level) and single-sentence sentiment classification (sentence-level).', '(2011), which has 1233 reports in the training set, 100 for development and 1000 for testing.', '(1), and these representations can be used as inputs to any classifier.', 'Finally, few-shot learning methods attempt to learn how to effectively train classifiers for new classes with only few labeled instances, using techniques commonly referred to as meta-learning (Snell et al., 2017; Yu et al., 2018).', 'We then use that function to embed all of the dataset text instances and use these text embeddings as inputs to the classifiers.', 'Incorporating more inductive bias is particularly important with little training data.', 'RA-CNN is the Rationale-Augmented CNN by Zhang et al.']\n",
      "\n",
      "max score: 0.3835616390283773\n",
      "******************************\n",
      "\n",
      "We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.\n",
      "\n",
      "['(2017), for example, learn an embedding function that is used to represent the input instances such that simple nearest-prototype classifiers can be trained effectively to recognise instances of a new target class after being exposed to a small number of labeled examples.', 'With this approach, they achieved state-of-the-art results on various NLP tasks, such as Named Entity Recognition (token-level) and single-sentence sentiment classification (sentence-level).', '(2011), which has 1233 reports in the training set, 100 for development and 1000 for testing.', '(1), and these representations can be used as inputs to any classifier.', 'Finally, few-shot learning methods attempt to learn how to effectively train classifiers for new classes with only few labeled instances, using techniques commonly referred to as meta-learning (Snell et al., 2017; Yu et al., 2018).', 'We then use that function to embed all of the dataset text instances and use these text embeddings as inputs to the classifiers.', 'Incorporating more inductive bias is particularly important with little training data.', 'RA-CNN is the Rationale-Augmented CNN by Zhang et al.']\n",
      "\n",
      "max score: 0.3835616390283773\n",
      "******************************\n",
      "\n",
      "Implicit feedback (e.g., clicks, dwell times, etc.) is an abundant source of data in human-interactive systems. While implicit feedback has many advantages (e.g., it is inexpensive to collect, user centric, and timely), its inherent biases are a key obstacle to its effective use. For example, position bias in search rankings strongly influences how many clicks a result receives, so that directly using click data as a training signal in Learning-to-Rank (LTR) methods yields sub-optimal results. To overcome this bias problem, we present a counterfactual inference framework that provides the theoretical basis for unbiased LTR via Empirical Risk Minimization despite biased data. Using this framework, we derive a Propensity-Weighted Ranking SVM for discriminative learning from implicit feedback, where click models take the role of the propensity estimator. In contrast to most conventional approaches to de-bias the data using click models, this allows training of ranking functions even in settings where queries do not repeat. Beyond the theoretical support, we show empirically that the proposed learning method is highly effective in dealing with biases, that it is robust to noise and propensity model misspecification, and that it scales efficiently. We also demonstrate the real-world applicability of our approach on an operational search engine, where it substantially improves retrieval performance.\n",
      "\n",
      "['Instead, the only data we have is the partial and biased feedback ct . To overcome this problem, we take an approach inspired by [33] and extend it to the dynamic ranking setting.', 'As the model of user behavior, we use the Position-based click model (PBM [19]), where the marginal probability that user ut examines an article only depends only on its position.', 'The key idea is to correct for the selection bias with which relevance labels are observed in ct using techniques from survey sampling and causal inference [28, 29].', 'Second, the ranking system is the arbiter of how much exposure each item receives, where exposure directly influences opinion (e.g.', 'We first evaluate whether training a personalized model using the de-biased R̂Reg(d |x) regression estimator improves ranking performance over a non-personalized model.', 'In addition to the theoretical justification of our approach, we also conducted an empirical evaluation1.', 'Machine learning methods underlie most ranking algorithms.', 'Even though the unbiased regression estimator R̂Reg(d |x) only has access to the partial feedback ct , it tracks the performance of Skyline.', 'Learning a model for pt is not part of our dynamic LTR problem, as the position-bias model is merely an input to our dynamic LTR algorithms.', '[26, 31]), click models (e.g.', 'To evaluate our method on a real-world preference data, we adopt the ML-20M dataset [25].', 'Numerous approaches based on preferences (e.g.', 'Using the definition the definition of DEτ in Eq.', 'Learning in our dynamic ranking setting is related to the conventional learning-to-rank algorithms such as LambdaRank, LambdaMART, RankNet, Softrank etc.', 'For example, in a video streaming service, the feedback may be the percentage the user watched of each video.', 'D-ULTR) provides substantially higher NDCG than the unbiased global ranking D-ULTR(Glob) and the Naive ranking.']\n",
      "\n",
      "max score: 0.4332344164189172\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.\n",
      "\n",
      "['We also show that using a stronger model such as PEGASUS we can achieve results that are on par with the state-of-the-art on both datasets.', 'We repeat this process until all sentences of the summary have been assigned to one document section.', 'In addition, the Pretraining with Extracted Gap-sentences for Abstractive SUmmarization Sequence-to-sequence (PEGASUS) [19] model is a Transformer encoder-decoder pre-trained on massive corpora of documents (Web and news articles) that has demonstrated great potential on various summarization benchmarks, including academic articles.', 'Our models were not extensively fine-tuned since this was outside the scope of our paper.', 'Usually it is way easier for extractive models to achieve higher ROUGE scores due to the way that ROUGE metrics are calculated.', 'Given an input sequence x = (x1, .', 'For the PEGASUS model, based on our experimental results there was a minimal number of repetitions within the section summaries generated by the model.', 'Gap Sentence Generation (GSG) is a self-supervised objective engineered specifically for abstractive summarization.', 'Finally, given the increased popularity and success of large pre-trained Transformers [29] in various NLP tasks, many recent methods employ Transformer models [6].', 'We propose a divide-and-conquer approach for the summarization of long documents.', 'Finally, results of PEGASUS and BigBird-PEGASUS are taken from [18].']\n",
      "\n",
      "max score: 0.4727272677685951\n",
      "******************************\n",
      "\n",
      "We propose SUSIE, a novel summarization method that can work with state-of-the-art summarization models in order to produce structured scientific summaries for academic articles. We also created PMC-SA, a new dataset of academic publications, suitable for the task of structured summarization with neural networks. We apply SUSIE combined with three different summarization models on the new PMC-SA dataset and we show that the proposed method improves the performance of all models by as much as 4 ROUGE points.\n",
      "\n",
      "['Here we describe the experiments we conducted with DANCER and the different summarization models on two different datasets in order to demonstrate the effectiveness of the method.', 'This is also a rather small dataset that is not suitable for neural summarization approaches.', 'These samples demonstrate the quality of the summaries we can produce using our proposed methods as well as directly compare the outputs produced by the different summarization models.', 'This work is concerned with the neural summarization of long documents, such as academic articles and financial reports.', 'Here we describe the different summarization models that we combined with DANCER for our experiments.', 'We propose a divide-and-conquer approach for the summarization of long documents.', 'Restrictions apply.', 'We presented DANCER, a novel summarization method for long documents.']\n",
      "\n",
      "max score: 0.5354330659557319\n",
      "******************************\n",
      "\n",
      "Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.\n",
      "\n",
      "['As there is no prior work doing few-shot and zero-shot evaluations on all the datasets we consider, and also the results in the few-shot setting might be influenced by sampling variability (especially with only 10 examples) (Bragg et al., 2021), we run the same experiments for the compared models five times with different random seeds (shared with all the models), with the public available checkpoints .8 BART (Lewis et al., 2020) an encoder-decoder transformer model pre-trained on the objective of reconstructing the corrupted documents in multiple ways, e.g.', 'While pre-trained transformers with general autoregressive or autoencoding language modeling objectives (Raffel et al., 2020; Lewis et al., 2020) perform well across a variety of tasks, utilizing pre-training objectives that are closer to the downstream summarization task have been shown to provide further gains.', 'In addition to this 15% masked sentences, following PEGASUS (Zhang et al., 2020), we also copy an additional 15% of the input sentences to the output without masking them in the input.', 'We show that despite its simplicity, PRIMER achieves superior performance compared with prior state-of-the-art pre-trained models, as well as dataset-specific models in both few-shot and full fine-tuning settings.', 'We conduct extensive experiments on 6 multidocument summarization datasets from 3 different domains.', 'As it is only evaluated on one multi-document summarization dataset (Multi-news), we rerun the model on all the datasets.', '(2020) evaluate three state-of-the-art models (BART, PEGASUS, T5) on several multi-document summarization datasets with low-resource settings, showing that despite the large improvement on single-document summarization, highly abstractive multi-document summarization remains challenging.', 'Our model outperforms all baselines on all of the datasets with 10 and 100 examples demonstrating the benefits of our pre-training strategy and input structure.', 'Text in red are the sentences with the highest Principle ROUGE scores.', 'Note that this dataset does not have any groundtruth summaries.', 'In this work, we show how efficient transformers can be pre-trained using a task-inspired pre-training objective for multi-document summarization.', 'PRIMER employs a new input structure for multi-document generation tasks and processes them with an efficient long-context transformer.', 'Each document is a scientific paper, and the summary is the corresponding abstract.', 'In this paper, we propose PRIMER a pre-trained model with focus on the downstream task of multidocument summarization.']\n",
      "\n",
      "max score: 0.42477875640657503\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n",
      "\n",
      "['Reformer models [23] replace the dot-product attention by locality-sensitive hashing, changing its complexity from O(T 2) to O(T ), where T is the sequence length.', 'In contrast to sparse information found in the tasks above, we now illustrate RelRNN and RelLSTM’s performance on tasks with densely distributed information on long sequences.', 'We note the Denoise task has a small number of relevant events and that not all tasks share this structure.', 'What can be said about gradient propagation?', 'The gradient norms of all attention models were stable, as expected from the results of Section 3.']\n",
      "\n",
      "max score: 0.4137930984482759\n",
      "******************************\n",
      "\n",
      "Query relevance ranking and sentence saliency ranking are the two main tasks in extractive query-focused summarization. Previous supervised summarization systems often perform the two tasks in isolation. However, since reference summaries are the trade-off between relevance and saliency, using them as supervision, neither of the two rankers could be trained well. This paper proposes a novel summarization system called AttSum, which tackles the two tasks jointly. It automatically learns distributed representations for sentences as well as the document cluster. Meanwhile, it applies the attention mechanism to simulate the attentive reading of human behavior when a query is given. Extensive experiments are conducted on DUC query-focused summarization benchmark datasets. Without using any hand-crafted features, AttSum achieves competitive performance. It is also observed that the sentences recognized to focus on the query indeed meet the query need.\n",
      "\n",
      "['Meanwhile, it applies the attention mechanism that tries to simulate human attentive reading behavior when a query is given.', 'It generates distributed representations for sentences as well as the document cluster.', 'It is a neural attention summarization system that tackles query relevance ranking and sentence salience ranking jointly.', 'This paper focuses on the task of query-focused extractive summarization.', 'While, words in query and document are often different in expression.', 'AttSum [4] is a summarization system which joints query relevance ranking and sentence saliency ranking with a neural attention model.', 'To evaluate our approach, we conduct experiments on the widely-used DUC 2005–2007 query-focused summarization benchmark datasets.']\n",
      "\n",
      "max score: 0.599999995030247\n",
      "******************************\n",
      "\n",
      "Query relevance ranking and sentence saliency ranking are the two main tasks in extractive query-focused summarization. Previous supervised summarization systems often perform the two tasks in isolation. However, since reference summaries are the trade-off between relevance and saliency, using them as supervision, neither of the two rankers could be trained well. This paper proposes a novel summarization system called AttSum, which tackles the two tasks jointly. It automatically learns distributed representations for sentences as well as the document cluster. Meanwhile, it applies the attention mechanism to simulate the attentive reading of human behavior when a query is given. Extensive experiments are conducted on DUC query-focused summarization benchmark datasets. Without using any hand-crafted features, AttSum achieves competitive performance. It is also observed that the sentences recognized to focus on the query indeed meet the query need.\n",
      "\n",
      "['2016 [6] applied the attention mechanism in a joint neural network model that can learn query relevance ranking and sentence saliency ranking simultaneously,and achieved competitive performance on DUC query-focused summarization benchmark datasets 2 .', 'The sentences of the original document will be feed into the BERT encoder sequentially, and be classified as if or not to be included in the summary.', 'We will focus on extractive summarization in this paper.', 'This automatic metric measures the overlap of 1-gram (R-1), bigrams (R-2) and the longest common subsequence between the model generated summaries and the reference summaries.', 'CNN/Daily Mail [8] dataset is the most widely used large scale dataset for summarization and reading comprehension.', 'As the best of our knowledge, 1 and consists of two main modules, BERT encoder.']\n",
      "\n",
      "max score: 0.3937823784198234\n",
      "******************************\n",
      "\n",
      "Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favorable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or `duet' performs significantly better than either neural network individually on a Web page ranking task, and also significantly outperforms traditional baselines and other recently proposed models based on neural networks.\n",
      "\n",
      "['In Section 7.4, we will discuss deep neural network models, such as [71, 141], that estimate relevance based on patterns of exact query term matches in the document.', 'The unlabelled document (or query) corpus is used to learn good text representations, and then these learnt representations are incorporated into an existing retrieval model or a query-document similarity metric.', 'However, as seen in the example of Figure 14, the errors made by embedding based models and exact matching models are typically different—and the combination of the two performs better than exact matching models alone [4, 58, 143].', 'On the other hand, a neural model focused on matching in the embedding space is unlikely to have a good representation for this rare term.', 'Telescoping evaluation Figure 14 highlights the distinct strengths and weaknesses of matching using local and distributed representations of terms for retrieval.', 'We present a tutorial on neural methods for information retrieval.', 'Siamese networks represent both the query and the document using single embedding vectors.', 'The model is a deep auto-encoder trained on unlabelled document corpus.', 'Salakhutdinov and Hinton [174] proposed one of the earliest deep neural models for ad-hoc retrieval.', 'Neural representation learning models share some commonalities with these traditional approaches.', 'In the context of neural models, distributed representations generally refer to learnt embeddings.']\n",
      "\n",
      "max score: 0.49789029057237993\n",
      "******************************\n",
      "\n",
      "Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favorable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or `duet' performs significantly better than either neural network individually on a Web page ranking task, and also significantly outperforms traditional baselines and other recently proposed models based on neural networks.\n",
      "\n",
      "['When training a neural model for this task, the ideal ranking of documents for a query q from the training dataset can be determined based on the relevance labels relq(d) associated with each document d ∈ D. In the pointwise approach, the neural model is trained to directly estimate relq(d), which can be a numeric value or a categorical label.', 'Figure 4.2 highlights the distinct strengths and weaknesses of matching using local and distributed representations of terms for retrieval.', 'The architecture consists of two models (model1 and model2) that project input1 and input2, respectively, to ~v1 and ~v2 in a common latent space.', '(2017a) have recently emphasized the importance of modelling lexical matches using deep neural networks.', 'This can be demonstrated by using a toy corpus, such as the one in Table 3.1.', 'Embedding based models often make different errors than exact matching models, and the combination of the two may be more effective.', 'DNNs for IR can learn text representations in situ, or use pre-trained embeddings.', 'Models that combine both performs best.', '“Learning semantic representations using convolutional neural networks for Web search”.', 'Introduction to information retrieval.', 'The position of these occurrences and the relationship with other terms in the document are ignored.', '“A convolutional neural network for modelling sentences”.', '“The analysis of permutations”.', 'The local model proposed by Diaz et al.', 'We have focused on retrieval of long and short text.']\n",
      "\n",
      "max score: 0.49193547916363173\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favorable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or `duet' performs significantly better than either neural network individually on a Web page ranking task, and also significantly outperforms traditional baselines and other recently proposed models based on neural networks.\n",
      "\n",
      "['The late combination models, on the other hand, separately learn a representation for query and document and then compute the relevance score using a matching function applied on the learned representations.', 'These deep neural ranking models (NRMs) often consider a single source of document description, such as document title [8, 23] or body text [5, 15].', 'The results also suggest that NRM-F performs better than neural ranking models with score aggregation.', 'Since in this paper we focus on the ad-hoc retrieval task, the only available information for the query is the query text.', '5To have a fair comparison, we only consider the distributed part of the model.', 'The local component of the duet model in [15] and the neural ranking models proposed in [5, 27] are the other examples for early combination models.', 'We multiply the representations for existing ield instances by one (means no change) and those for the missing instances by zero.', 'Deep neural networks have recently shown promise in the adhoc retrieval task.', 'In this paper, we focus on late combination models and propose a neural ranking model that takesmultiple ields of document into account.', 'These hyper-parameters are selected for each ield individually based on a validation set.', 'As shown in Table 4, NRM-F signiicantly outperforms all the baselines.']\n",
      "\n",
      "max score: 0.5315315266488111\n",
      "******************************\n",
      "\n",
      "In recent years, a great many methods of learning from multi-view data by considering the diversity of different views have been proposed. These views may be obtained from multiple sources or different feature subsets. In trying to organize and highlight similarities and differences between the variety of multi-view learning approaches, we review a number of representative multi-view learning algorithms in different areas and classify them into three groups: 1) co-training, 2) multiple kernel learning, and 3) subspace learning. Notably, co-training style algorithms train alternately to maximize the mutual agreement on two distinct views of the data; multiple kernel learning algorithms exploit kernels that naturally correspond to different views and combine kernels either linearly or non-linearly to improve learning performance; and subspace learning algorithms aim to obtain a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Though there is significant variance in the approaches to integrating multiple views to improve learning performance, they mainly exploit either the consensus principle or the complementary principle to ensure the success of multi-view learning. Since accessing multiple views is the fundament of multi-view learning, with the exception of study on learning a model from multiple views, it is also valuable to study how to construct multiple views and how to evaluate these views. Overall, by exploring the consistency and complementary properties of different views, multi-view learning is rendered more effective, more promising, and has better generalization ability than single-view learning.\n",
      "\n",
      "['Multi-view learning approaches, which utilize the consistency and complementary information in different views, demonstrates more effective, more promising, and better generalization ability than the single-view counterparts in many problems [29].', 'This 24914 VOLUME 9, 2021 algorithm is a co-training style multi-view learning algorithm where the available attributes of data objects are divided into two distinct views.', 'Inspired by Bickel and Scheffer’s work, we also utilize the consensus principle, which aims to maximize the agreement on multiple views, to gain the final clustering result [29], [31].', 'The existing multiview learning algorithms fall into one of the following three types: co-training, multiple kernel learning, and subspace learning [29].', 'With the success of multi-view learning, multi-view clustering has attracted more and more attention in recent years.', 'These two types of attributes have different domains of values, and they coexist in the clusters and data objects.', 'To evaluate the algorithm’s performance, we assessed the proposed Multi-view K-Prototypes algorithm on four datasets.', 'In order to assess the gained clustering results, we employ these two measures in this research.', 'These algorithms are either single view ones or not designed for data with both numeric and categorical attributes.', 'Recalculate the similarity between data objects and the prototypes after all data objects have been assigned.']\n",
      "\n",
      "max score: 0.4548736412125794\n",
      "******************************\n",
      "\n",
      "This work investigates continual learning of two segmentation tasks in brain MRI with neural networks. To explore in this context the capabilities of current methods for countering catastrophic forgetting of the first task when a new one is learned, we investigate elastic weight consolidation, a recently proposed method based on Fisher information, originally evaluated on reinforcement learning of Atari games. We use it to sequentially learn segmentation of normal brain structures and then segmentation of white matter lesions. Our findings show this recent method reduces catastrophic forgetting, while large room for improvement exists in these challenging settings for continual learning.\n",
      "\n",
      "['They investigate elastic weight consolidation, a method based on Fisher information to sequentially learn segmentation of normal brain structures and then segmentation of white matter lesions and demonstrate this method reduces catastrophic forgetting, but acknowledge there is a large room for improvement for the challenging setting of continual learning.', '(2018) investigate continual learning of two MRI segmentation tasks with neural networks for countering catastrophic forgetting of the first task when a new one is learned.', 'Hence, it would be desirable to combine these two in future work.']\n",
      "\n",
      "max score: 0.7123287621570652\n",
      "******************************\n",
      "\n",
      "Semantic matching, which aims to determine the matching degree between two texts, is a fundamental problem for many NLP applications. Recently, deep learning approach has been applied to this problem and significant improvements have been achieved. In this paper, we propose to view the generation of the global interaction between two texts as a recursive process: i.e. the interaction of two texts at each position is a composition of the interactions between their prefixes as well as the word level interaction at the current position. Based on this idea, we propose a novel deep architecture, namely Match-SRNN, to model the recursive matching structure. Firstly, a tensor is constructed to capture the word level interactions. Then a spatial RNN is applied to integrate the local interactions recursively, with importance determined by four types of gates. Finally, the matching score is calculated based on the global interaction. We show that, after degenerated to the exact matching scenario, Match-SRNN can approximate the dynamic programming process of longest common subsequence. Thus, there exists a clear interpretation for Match-SRNN. Our experiments on two semantic matching tasks showed the effectiveness of Match-SRNN, and its ability of visualizing the learned matching structure.\n",
      "\n",
      "['This spatial GRU model is good at modeling the matching between two pieces of texts based on primitive word features, and has shown better performances as compared with other deep matching models [34].', 'Finally, we use our proposed implementation, i.e., a spatial GRU over the Sxor and Scos tensors, which can encode exact matching signals, semantic matching signals, proximity and term importance.', 'In this paper, we have introduced a hierarchical neural matching model to capture the diverse relevance patterns in ad-hoc retrieval.', 'Based on the signals at each position, they proposed three strategies, namely best position strategy, multi-position strategy and multi-σ strategy, for final relevance assessment.', 'Firstly, the termlevel interaction matrix is constructed based on the term vectors from the query-passage pair.', 'We adopt three types of baselines for comparison, including traditional retrieve models, learning to rank models and deep matching models.', 'In this section, we conduct experiments to demonstrate the effectiveness of our proposed model on benchmark collections.', 'For DeepRank4, we use the code released by their authors.', 'Themodel consists of two components, namely local matching layer and global decision layer.', 'The two cases show that there are indeed quite diverse relevance patterns in real-world retrieval scenario, and our HiNT model can capture these diverse relevance patterns successfully.', 'Flexible strategies are applied in this layer to model diverse relevance patterns.']\n",
      "\n",
      "max score: 0.41509433466714135\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper presents Macquarie University's participation to the BioASQ Synergy Task, and BioASQ9b Phase B. In each of these tasks, our participation focused on the use of query-focused extractive summarisation to obtain the ideal answers to medical questions. The Synergy Task is an end-to-end question answering task on COVID-19 where systems are required to return relevant documents, snippets, and answers to a given question. Given the absence of training data, we used a query-focused summarisation system that was trained with the BioASQ8b training data set and we experimented with methods to retrieve the documents and snippets. Considering the poor quality of the documents and snippets retrieved by our system, we observed reasonably good quality in the answers returned. For phase B of the BioASQ9b task, the relevant documents and snippets were already included in the test data. Our system split the snippets into candidate sentences and used BERT variants under a sentence classification setup. The system used the question and candidate sentence as input and was trained to predict the likelihood of the candidate sentence being part of the ideal answer. The runs obtained either the best or second best ROUGE-F1 results of all participants to all batches of BioASQ9b. This shows that using BERT in a classification setup is a very strong baseline for the identification of ideal answers.\n",
      "\n",
      "['The “MQ” team [23] focused on the question answering component of the task, section ideal answers using one of their systems that participated in BioASQ 8b [22] Phase B For document retrieval, they used the top documents returned by the API provided by BioASQ.', 'In task Synergy the participating systems were expected to retrieve documents and snippets, as in phase A of task B, and, at the same time, provide answers for some of these questions, as in phase B of task B.', 'The results of Task 9a reveal that several participating systems manage to outperform the strong baselines in all test batches and considering either the flat or the hierarchical measures.', 'The first team (“MQ”) [23] competed with five systems which are based on the use of BERT variants in a classification setting.', 'This model was also used as an initial step for their systems in phase B, in order to re-rank candidate sentences.', 'In contrast to task B, it is possible that no answer exists for some questions.', 'In this task, the system with the best performance in a test set gets rank 1.0 for this test set, the second best rank 2.0 and so on.', 'Similarly to the previous years, for each test set, participants are required to submit their answers in 21 hours.', 'In run 4, they experimented with a variant of document retrieval based on an independent retrieval system, tuned with the BioASQ data.', 'For summary questions, they utilized both extractive and abstractive methods.', 'Similarly, for snippet retrieval task, we use the same method with the focus in sentence.']\n",
      "\n",
      "max score: 0.5088339173379617\n",
      "******************************\n",
      "\n",
      "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and release our data and code in hope that this work will promote advances in summarization in the multi-document setting.\n",
      "\n",
      "['However, a fundamental limitation of those supervised methods is that their success heavily depends on the availability of large training corpora with human-generated high-quality summaries [5, 7, 12, 16], which are extremely costly to produce and difficult to obtain.', 'The resulting model achieves competitive results with state-of-the-art unsupervised multi-document extractive summarization methods.', 'In this paper, we focus on improving the state of the art in unsupervised extractive text summarization.', 'Experimental results on three summarization benchmark datasets showed the effectiveness of our proposed methods.', 'To avoid the expensive cost of data annotation, unsupervised neural learning for text summarization has received an increasing amount of attentions recently [9, 18, 19, 22].']\n",
      "\n",
      "max score: 0.3936170163128113\n",
      "******************************\n",
      "\n",
      "This paper presents a Kernel Entity Salience Model (KESM) that improves text understanding and retrieval by better estimating entity salience (importance) in documents. KESM represents entities by knowledge enriched distributed representations, models the interactions between entities and words by kernels, and combines the kernel scores to estimate entity salience. The whole model is learned end-to-end using entity salience labels. The salience model also improves ad hoc search accuracy, providing effective ranking features by modeling the salience of query entities in candidate documents. Our experiments on two entity salience corpora and two TREC ad hoc search datasets demonstrate the effectiveness of KESM over frequency-based and feature-based methods. We also provide examples showing how KESM conveys its text understanding ability learned from entity salience to search.\n",
      "\n",
      "['This is due to two reasons:1) using entity information to represent the whole query will lose a part of information out of the extracted entities and there are also a few queries without any entities (See Figure 4).', 'We systematically analyze the effectiveness of topic information in different ranking scenarios, thereby providing a solid understanding of how to effectively utilize topic information with neural retrieval models.', '[44] modeled entity salience in document by modeling the interactions between entities and words.', 'The parameters of the whole model are learned to better estimate the final relevance.', '• KESM [44] (S+K): KESM is proposed to model the salience of query entities in candidate documents.', 'It shows that retrieval models trained on click labels are also effective for human evaluation.', 'Extensive experiments demonstrate the effectiveness of the proposed framework and its advantages in different scenarios.', 'RM(q,d) is replaced by the scores from KESM and EDRM.', 'It uses kernel pooling to model the interactions of query entities with the entities and words in the document.', 'Our study systematically analyzes the effectiveness of topic information in different ranking scenarios, providing a solid understanding of how to effectively utilize topic information for neural retrieval models.']\n",
      "\n",
      "max score: 0.5106382929289273\n",
      "******************************\n",
      "\n",
      "Visual representations are defined in terms of minimal sufficient statistics of visual data, for a class of tasks, that are also invariant to nuisance variability. Minimal sufficiency guarantees that we can store a representation in lieu of raw data with smallest complexity and no performance loss on the task at hand. Invariance guarantees that the statistic is constant with respect to uninformative transformations of the data. We derive analytical expressions for such representations and show they are related to feature descriptors commonly used in computer vision, as well as to convolutional neural networks. This link highlights the assumptions and approximations tacitly assumed by these methods and explains empirical practices such as clamping, pooling and joint normalization.\n",
      "\n",
      "['We refer the reader to [48] for a more in depth discussion about optimal visual representations and minimal sufficient statistics.', 'Even in the case of these optimal traces, there are three regimes of performance we can consider that are depicted in Fig.', '(Optimal Representation of a Task) For a task T whose goal is to predict a semantic label y from the input data x, the optimal representation z∗ encoded from x is the minimal sufficient statistic with respect to y.', 'We use INCE as a neural proxy for I , and note it depends on network architectures.', 'These methods seek representations of the world that are invariant to a family of viewing conditions.', 'This is performed by freezing the backbone and training a linear task-specific head.']\n",
      "\n",
      "max score: 0.37234042055964245\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A number of backpropagation-based approaches such as DeConvNets, vanilla Gradient Visualization and Guided Backpropagation have been proposed to better understand individual decisions of deep convolutional neural networks. The saliency maps produced by them are proven to be non-discriminative. Recently, the Layer-wise Relevance Propagation (LRP) approach was proposed to explain the classification decisions of rectifier neural networks. In this work, we evaluate the discriminativeness of the generated explanations and analyze the theoretical foundation of LRP, i.e. Deep Taylor Decomposition. The experiments and analysis conclude that the explanations generated by LRP are not class-discriminative. Based on LRP, we propose Contrastive Layer-wise Relevance Propagation (CLRP), which is capable of producing instance-specific, class-discriminative, pixel-wise explanations. In the experiments, we use the CLRP to explain the decisions and understand the difference between neurons in individual classification decisions. We also evaluate the explanations quantitatively with a Pointing Game and an ablation study. Both qualitative and quantitative evaluations show that the CLRP generates better explanations than the LRP. The code is available.\n",
      "\n",
      "['For the qualitative evaluation, we compare SGLRP to the related methods of LRP [3] and CLRP [8] as well as a stateof-the-art pixel-wise visualization method, Guided GradCAM [26].', 'In this paper, we proposed a novel method for understanding the contribution that regions in the input have on classification predictions.', 'The proposed Softmax-Gradient Layer-wise Relevance Propagation (SGLRP) is an extension of LRP which adds the ability for class discrimination by subtracting the relevance from non-target classes using the gradient of softmax.', '• We demonstrate through quantitative evaluations that the proposed method performs better than other LRP-based visualizations.', 'Deep Taylor Decomposition (DTD) [19] based models such as LRP [3] and Contrastive LRP (CLRP) [8] use both the gradient and the input to propagate the relevance of the output backwards through the trained network.', 'There are also many other visualization and explanation methods for neural networks.', 'The Pointing Game was proposed by Zhang et al.', 'We first perform a qualitative evaluation based on the relevance maps produced by comparative pixel-wise methods.', 'The proposed method is a general visualization method and can be used with most neural network models.', 'Specifically, we propose an extension of LRP called Softmax-Gradient Layer-wise Relevance Propagation (SGLRP).', '[20] optimize generated images to maximally activate particular neurons.', '(8) This balance between the target class and the other classes ensures that the relevance of the non-target classes do not overpower the relevance of the target class.', 'A key feature of SGLRP is the ability to get the pixel-wise relevance maps for a target class.', 'Thus, by measuring the change in ŷt, it is possible to quantitatively evaluate the different visualization methods.']\n",
      "\n",
      "max score: 0.4427480867440709\n",
      "******************************\n",
      "\n",
      "When AI systems interact with humans in the loop, they are often called on to provide explanations for their plans and behavior. Past work on plan explanations primarily involved the AI system explaining the correctness of its plan and the rationale for its decision in terms of its own model. Such soliloquy is wholly inadequate in most realistic scenarios where the humans have domain and task models that differ significantly from that used by the AI system. We posit that the explanations are best studied in light of these differing models. In particular, we show how explanation can be seen as a \"model reconciliation problem\" (MRP), where the AI system in effect suggests changes to the human's model, so as to make its plan be optimal with respect to that changed human model. We will study the properties of such explanations, present algorithms for automatically computing them, and evaluate the performance of the algorithms.\n",
      "\n",
      "['• Approaches: Past work on explanations primarily involved the AI system explaining the correctness of its plan and the rationale for its decision in terms of its own model [48].', 'The underlying technologies are used to develop machines that can replicate human actions.', 'Such properties characterize the explanation.', 'Explanation is usually to fine-grained to be properly integrated by humans.', '• Limitations: Although the latter models extract information from a large poll of data, such systems do not explain their actions and justify their decisions [39].', 'Constraints are defined on a finite domain.']\n",
      "\n",
      "max score: 0.4761904712141317\n",
      "******************************\n",
      "\n",
      "We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.\n",
      "\n",
      "['We compare the performance of the ETM with two other document models: latent Dirichlet allocation (LDA) and the neural variational document model (NVDM).', 'Figures 2 and 3 show topics from a 300-topic ETM of The New York Times.', 'We also use data subsampling to handle large collections of documents (Hoffman et al., 2013).', 'In this paper, we develop the embedded topic model (ETM), a topic model for word embeddings.', '(We set the prior hyperparameters to 1.)', 'Thus it does not require pre-fitted embeddings and, indeed, can learn embeddings as part of its inference process.', 'We studied the performance of the ETM against several document models.', 'LDA is a hierarchical probabilistic model that represents each topic as a distribution over terms and represents each document as a mixture of the topics.', 'As for most topic models, the posterior of the topic proportions is intractable to compute.', 'The minibatch size is 1,000 documents.', 'In traditional topic modeling, each topic is a full distribution over the vocabulary.']\n",
      "\n",
      "max score: 0.48936169722329115\n",
      "******************************\n",
      "\n",
      "We propose a recurrent RL agent with an episodic exploration mechanism that helps discovering good policies in text-based game environments. We show promising results on a set of generated text-based games of varying difficulty where the goal is to collect a coin located at the end of a chain of rooms. In contrast to previous text-based RL approaches, we observe that our agent learns policies that generalize to unseen games of greater difficulty.\n",
      "\n",
      "['Testing agents on unseen games (within a difficulty level) is uncommon in prior RL work, where it is standard to train and test on a single game instance.', 'We show that GATA outperforms strong baselines, including text-based models with recurrent policies.', 'To enable GATA to scale and generalize to multiple games, we adapt standard deep Q-Learning by sampling a new game from the set of training games to collect an episode.']\n",
      "\n",
      "max score: 0.44247787112538184\n",
      "******************************\n",
      "\n",
      "Interactive NLP is a promising paradigm to close the gap between automatic NLP systems and the human upper bound. Preference-based interactive learning has been successfully applied, but the existing methods require several thousand interaction rounds even in simulations with perfect user feedback. In this paper, we study preference-based interactive summarisation. To reduce the number of interaction rounds, we propose the Active Preference-based ReInforcement Learning (APRIL) framework. APRIL uses Active Learning to query the user, Preference Learning to learn a summary ranking function from the preferences, and neural Reinforcement Learning to efficiently search for the (near-)optimal summary. Our results show that users can easily provide reliable preferences over summaries and that APRIL outperforms the state-of-the-art preference-based interactive method in both simulation and real-user experiments.\n",
      "\n",
      "['In another approach, an active preference-based reinforcement learning framework is proposed to query the user, to learn a summary ranking function from the user’s preferences and to efficiently search for the best summary (Gao et al., 2019).', 'It can be also observed that when there is no interaction between the system and the users, the number of users with medium or less satisfaction rises.', 'For receiving the user’s feedback and displaying the results through the interaction steps, a simple user interface has been designed as shown in Figures 5–8.', 'For this purpose, the method proposed in Bayatmakou et al.', 'Figure 12 shows the result of our experiments over 45 different summaries.', 'Then, we study the convergence of GA in the sentence interaction phase.', 'Generally, existing studies explore interactive summarization tasks by participating the user in various methods to present a user-oriented summary.']\n",
      "\n",
      "max score: 0.43781094030147766\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We present \\textit{AutoExtend}, a system to learn embeddings for synsets and lexemes. It is flexible in that it can take any word embeddings as input and does not need an additional training corpus. The synset/lexeme embeddings obtained live in the same vector space as the word embeddings. A sparse tensor formalization guarantees efficiency and parallelizability. We use WordNet as a lexical resource, but AutoExtend can be easily applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks.\n",
      "\n",
      "['AutoExtend is an auto-encoder that relies on the relations present in WordNet to learn embeddings for senses and lexemes.', 'The advantage of their method is that it is flexible: it can take any set of word embeddings and any lexical database as input and produces embeddings of senses and lexemes, without requiring any extra training data.', 'Our system makes use of a combination of sense embeddings, context embeddings, and gloss embeddings.', 'They lie within the same vector space as the pre-trained word embeddings by Mikolov et al.', 'We use the method of Miller et al.', 'It is therefore not a reliable source for computing the most frequent sense.']\n",
      "\n",
      "max score: 0.49645389572154314\n",
      "******************************\n",
      "\n",
      "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.\n",
      "\n",
      "['[2019], EXBERT provides insights into both the attention and the token embeddings for the user-defined model and corpus by probing whether the representations capture metadata such as linguistic features or positional information.', 'We call these heads positional heads in that they detect an offset from the current token [Clark et al., 2019].', 'Even at this early layer, it has learned to attend to the direct object (DOBJ) of that verb, a dependency that Clark et al.', 'First, the corpus is split by sentence, and its tokens are labeled for desired metadata (e.g., POS, DEP, NER).', 'Moreover, Transformer models have also had much success as autoregressively trained language models that can be used for generation tasks [Radford et al., 2019, Keskar et al., 2019].', 'Since there is no information in the original vector embedding of “[MASK]”, BERT must rely on its internal representations to fill in the missing tokens.', 'For instance, Clark et al.', 'We demonstrate the applicability of EXBERT to a specific case study for a BERT model across the Wizard of Oz corpus.', 'Similar to the static analysis by Clark et al.', 'Their outputs h(0), .', 'This also allows the tool to apply the same corpus to different transformer models that may require different tokenization.', 'Transformer models typically use n of these self attention heads in parallel.', 'We can observe that there is no meaningful linguistic information encoded in the mask’s embedding at this layer.']\n",
      "\n",
      "max score: 0.38661709540830014\n",
      "******************************\n",
      "\n",
      "Response retrieval is a subset of neural ranking in which a model selects a suitable response from a set of candidates given a conversation history. Retrieval-based chat-bots are typically employed in information seeking conversational systems such as customer support agents. In order to make pairwise comparisons between a conversation history and a candidate response, two approaches are common: cross-encoders performing full self-attention over the pair and bi-encoders encoding the pair separately. The former gives better prediction quality but is too slow for practical use. In this paper, we propose a new cross-encoder architecture and transfer knowledge from this model to a bi-encoder model using distillation. This effectively boosts bi-encoder performance at no cost during inference time. We perform a detailed analysis of this approach on three response retrieval datasets.\n",
      "\n",
      "['propose a new cross-encoder architecture and transfer knowledge from this model to a bi-encoder model for fast retrieval (Tahami et al., 2020).', 'The number of parameters and inference time are also given in Table 3 to facilitate the study of tradeoffs between model complexity and effectiveness.', 'Experiments on two ad-hoc retrieval benchmarks demonstrate PARADE’s effectiveness over such methods.', 'We use mean squared error as the distilling objective, which has been shown to work effectively (Tahami et al., 2020; Tang et al., 2019).', 'In all rounds, we employ the full PARADE model.', 'Knowledge distillation on PARADE boosts the performance of smaller PARADE models while substantially reducing their parameters.', 'Performance is measured in terms of the P@20 and nDCG@20 ranking metrics using trec eval5.', 'The statistics of these two datasets are shown in Table 1.']\n",
      "\n",
      "max score: 0.43434342934343445\n",
      "******************************\n",
      "\n",
      "Data augmentation is a critical component of training deep learning models. Although data augmentation has been shown to significantly improve image classification, its potential has not been thoroughly investigated for object detection. Given the additional cost for annotating images for object detection, data augmentation may be of even greater importance for this computer vision task. In this work, we study the impact of data augmentation on object detection. We first demonstrate that data augmentation operations borrowed from image classification may be helpful for training detection models, but the improvement is limited. Thus, we investigate how learned, specialized data augmentation policies improve generalization performance for detection models. Importantly, these augmentation policies only affect training and leave a trained model unchanged during evaluation. Experiments on the COCO dataset indicate that an optimized data augmentation policy improves detection accuracy by more than +2.3 mAP, and allow a single inference model to achieve a state-of-the-art accuracy of 50.7 mAP. Importantly, the best policy found on COCO may be transferred unchanged to other detection datasets and models to improve predictive accuracy. For example, the best augmentation policy identified with COCO improves a strong baseline on PASCAL-VOC by +2.7 mAP. Our results also reveal that a learned augmentation policy is superior to state-of-the-art architecture regularization methods for object detection, even when considering strong baselines. Code for training with the learned policy is available online at https://github.com/tensorflow/tpu/tree/master/models/official/detection\n",
      "\n",
      "['Training a machine learning model with a learned data augmentation policy may significantly improve image classification [3, 17, 13], object detection [45], model robustness [23, 39, 28], and semi-supervised learning for image classification [37].', 'Unlike architecture search [47], all of these improvements in predictive performance incur no additional computational cost at inference time since data augmentation is only used during training.', 'From Figure 1 (c) we observe that models trained on smaller training dataset sizes may gain more improvement from data augmentation (e.g.', 'Our goal is to demonstrate the relative benefits of employing this method over previous learned augmentation methods; the RandAugment model and the baseline model do not differ in any setting other than the data augmentation strategy.', 'Data augmentation has played a central role in the training of deep vision models.', 'For example, it was not clear if the optimal hyperparameters found on the proxy task are also optimal for the actual task.', 'In this work, we demonstrated that previous methods of learned augmentation suffers from systematic drawbacks.', 'Learned augmentation policies have improved object detection and lead to state-of-the-art results [45].', 'At first glance, this may disagree with the expectation that smaller datasets require stronger regularization.', 'We name our method RandAugment because for each image it uniformly samples from all available data augmentation operations in the search space.', 'how many degrees to rotate an image) of each operation in the search space.', 'The squares indicate the distortion magnitude that achieves the highest accuracy.', 'On the other hand, this improved technology can also be used by bad actors.']\n",
      "\n",
      "max score: 0.4812499951033204\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community detection and analysis is an important methodology for understanding the organization of various real-world networks and has applications in problems as diverse as consensus formation in social communities or the identification of functional modules in biochemical networks. Currently used algorithms that identify the community structures in large-scale real-world networks require a priori information such as the number and sizes of communities or are computationally expensive. In this paper we investigate a simple label propagation algorithm that uses the network structure alone as its guide and requires neither optimization of a pre-defined objective function nor prior information about the communities. In our algorithm every node is initialized with a unique label and at every step each node adopts the label that most of its neighbors currently have. In this iterative process densely connected groups of nodes form a consensus on a unique label to form communities. We validate the algorithm by applying it to networks whose community structures are known. We also demonstrate that the algorithm takes an almost linear time and hence it is computationally less expensive than what was possible so far.\n",
      "\n",
      "['On the whole, the main contributions of this newly proposed method are threefold: This paper proposes a new method based on label propagation algorithm and spreading activation process in order to improve community detection in overlapping dynamic social networks.', 'In part 3, as we use breadth-first search for each node of the graph, it totally requires O(n*m).', 'A community in a network is a set of densely connected nodes that contains weaker connections to other nodes of the network [2].', 'Nodes with the approximately same activation value labels form communities.', 'Then, each sender node chooses one of its labels whose activation value is greater than its other labels.', 'This method traced the community structure and applied various strategies based on nodes and edges added or removed.', 'A large number of community detection methods such as the proposed method ignores the direction of the edges in the graph.', 'Discovering communities, one of the important structures of social networks, is certainly a matter of debate.', 'The time complexity of this method in sparse networks is low while the accuracy of that is also low.', 'Funding The research was supported by University of Isfahan.', 'In this method, each node belongs to at most one community; therefore it cannot detect overlapping communities.', 'Each node is initialized with an activation value, where that of the source nodes be one and that of the other nodes be zero.', 'The activation value of each node represents the amount of activation of the node in order to propagate information.', 'Next, its neighbors are extracted and considered as the sender nodes.', 'In part 2, two weighting algorithms are applied.']\n",
      "\n",
      "max score: 0.4222222173111111\n",
      "******************************\n",
      "\n",
      "We propose an efficient method to learn deep local descriptors for instance-level recognition. The training only requires examples of positive and negative image pairs and is performed as metric learning of sum-pooled global image descriptors. At inference, the local descriptors are provided by the activations of internal components of the network. We demonstrate why such an approach learns local descriptors that work well for image similarity estimation with classical efficient match kernel methods. The experimental validation studies the trade-off between performance and memory requirements of the state-of-the-art image search approach based on match kernels. Compared to existing local descriptors, the proposed ones perform better in two instance-level recognition tasks and keep memory requirements lower. We experimentally show that global descriptors are not effective enough at large scale and that local descriptors are essential. We achieve state-of-the-art performance, in some cases even with a backbone network as small as ResNet18.\n",
      "\n",
      "['Testing examples do not necessarily come from the same categories as the training ones, as some form of open set recognition, and classification is in practice performed [37, 58] by verification that a query and database image pair come from the same class.', 'Even though the activations are treated as local descriptors at inference time, the network is trained with image level annotations only.', 'An inverted-file structure is used for efficient similarity estimation between a query image and each image of the reference set.', 'The multigrain descriptor [7] is an early example of global image embedding based largely on data augmentation.', 'The 500k pairs with the highest confidence score are used to estimate the evaluation metric.', 'We additionally make sure that specialized approaches, such as optical character recognition (OCR) and face recognition do not significantly improve performance.', 'The code for baseline methods (Section 5) is provided.', 'Use cases.', 'Such approach is less effective for detecting partial copies.', 'the ones in Phase I.', 'HOW deep local descriptors with ASMK.', 'Recently, more attention is paid on ILR for classification at large scale [37, 58], which bears similarities to the task of our work.']\n",
      "\n",
      "max score: 0.4237288086889544\n",
      "******************************\n",
      "\n",
      "We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle & Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.\n",
      "\n",
      "['This agrees with the observations made in works on model compression: sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization (Zhu and Gupta, 2017; Gale et al., 2019).', 'For English-Russian, we perform additional experiments using the publicly available OpenSubtitles2018 corpus (Lison et al., 2018) to evaluate the impact of domains on our results.', 'We adapt LRP to the Transformer model to calculate relevance that measures the association degree between two arbitrary neurons in neural networks.', 'However, we leave proper analysis of this to future work.', 'These observations are similar for both datasets we use.', 'Heads from all layers are ordered by their function.']\n",
      "\n",
      "max score: 0.47441859966944294\n",
      "******************************\n",
      "\n",
      "Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks such as language modeling (Linzen et al., 2016) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures---recurrent versus non-recurrent---with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose.\n",
      "\n",
      "['An alternative method to study the ability of language models and machine translation models to capture hierarchical information is to test their sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Sennrich, 2017; Tang et al., 2018).', 'Examples of attention maps for this head for models trained on WMT data with different target languages are shown in Figure 5.', 'The Transformer (Vaswani et al., 2017) has become the dominant modeling paradigm in neural machine translation.', 'In this work, we concentrate primarily on encoder self-attention.', 'This indicates that these functions are indeed the most important.']\n",
      "\n",
      "max score: 0.4528301836952652\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical structure is ubiquitous in data across many domains. There are many hierarchical clustering methods, frequently used by domain experts, which strive to discover this structure. However, most of these methods limit discoverable hierarchies to those with binary branching structure. This limitation, while computationally convenient, is often undesirable. In this paper we explore a Bayesian hierarchical clustering algorithm that can produce trees with arbitrary branching structure at each node, known as rose trees. We interpret these trees as mixtures over partitions of a data set, and use a computationally efficient, greedy agglomerative algorithm to find the rose trees which have high marginal likelihood given the data. Lastly, we perform experiments which demonstrate that rose trees are better models of data than the typical binary trees returned by other hierarchical clustering algorithms.\n",
      "\n",
      "['In this paper, we use textual data as a guiding example to illustrate how the interactive steering of hierarchical clustering can be achieved.', 'An example is evolutionary Bayesian rose trees [9], in which the authors consider the structure extracted at the previous time as the constraints and apply them when clustering the documents at the current time point.', 'We leverage this algorithm to find a constraint tree that consists of good paths by considering documents as ants.', 'To better incorporate domain knowledge, constraint-based methods have been developed.', 'This constraint tree only consists of a subset of the documents that are mapped to part of the knowledge base with high confidence.', 'However, the binary structure limits its application.', 'If we consider each document independently, we could easily obtain a constraint tree with many nodes scattered across the knowledge base.', 'We use the hierarchies in Fig.', 'Hierarchical clustering consists of two components.', 'The most computationally expensive step is accuracy calculation (Eq.', 'Four datasets are used in the evaluation.', 'This function allows the user to explore more nodes while keeping the nodes of interest in context.']\n",
      "\n",
      "max score: 0.44131454911062623\n",
      "******************************\n",
      "\n",
      "In some of object recognition problems, labeled data may not be available for all categories. Zero-shot learning utilizes auxiliary information (also called signatures) describing each category in order to find a classifier that can recognize samples from categories with no labeled instance. In this paper, we propose a novel semi-supervised zero-shot learning method that works on an embedding space corresponding to abstract deep visual features. We seek a linear transformation on signatures to map them onto the visual features, such that the mapped signatures of the seen classes are close to labeled samples of the corresponding classes and unlabeled data are also close to the mapped signatures of one of the unseen classes. We use the idea that the rich deep visual features provide a representation space in which samples of each class are usually condensed in a cluster. The effectiveness of the proposed method is demonstrated through extensive experiments on four public benchmarks improving the state-of-the-art prediction accuracy on three of them.\n",
      "\n",
      "['Transductive setting assumption was used in (Shojaee and Baghshah, 2016), and they used both labeled samples of seen classes and unlabeled instances of unseen classes to learn a proper representation of labels in the space of deep visual features in which samples of each class are usually condensed in a cluster.', '(2013) learned a deep learning model to map image close to semantic word vectors corresponding to their classes, and this embedding manner could be used to distinguish whether an image is of a seen or unseen class.', 'And FiLM is demonstrated that can generalize well to challenging, new data from few examples or even zero-shot settings.', 'There also exist some works using novel generative model.', '(2017a) proposed a unified multi-view subspace learning method for CCA using the graph embedding framework for visual recognition and cross-modal retrieval.', 'In this section, we will give an overview on the techniques on experience learning.', '(2018) learned an object level representation and exploited rich object-level information to infer image similarity.', 'The middle-level representation may be the result after a mathematical transformation (say, Fourier transformation).', 'we will introduce them in the remainder of this section.', 'We will present several techniques of rectification in this section.']\n",
      "\n",
      "max score: 0.48535564360918054\n",
      "******************************\n",
      "\n",
      "Zero-Shot Learning (ZSL) is achieved via aligning the semantic relationships between the global image feature vector and the corresponding class semantic descriptions. However, using the global features to represent fine-grained images may lead to sub-optimal results since they neglect the discriminative differences of local regions. Besides, different regions contain distinct discriminative information. The important regions should contribute more to the prediction. To this end, we propose a novel stacked semantics-guided attention (S2GA) model to obtain semantic relevant features by using individual class semantic features to progressively guide the visual features to generate an attention map for weighting the importance of different local regions. Feeding both the integrated visual features and the class semantic features into a multi-class classification architecture, the proposed framework can be trained end-to-end. Extensive experimental results on CUB and NABird datasets show that the proposed approach has a consistent improvement on both fine-grained zero-shot classification and retrieval tasks.\n",
      "\n",
      "['(Yu et al., 2018) further applied attention mechanism to generating an attention map for weighting the importance of different local regions and then integrated both the local and global features to obtain more discriminative representations for fine-grained ZSL.', '(2014) investigated a three-view CCA framework that incorporates the dependence of visual features and text on the underlying image semantics for retrieval tasks.', 'The intuition they considered is that learning an explicit encoding function between different modalities may be easily spoiled.', '2) Learning a probabilistic distribution for each seen class and extrapolating to unseen class distributions using the class-attribute information.', 'Here we call this learning manner experience learning yet for unified descriptions.', 'This approach directly learns a mapping function between the visual feature space and the semantic embedding space.', '(2017) proposed an embedding model jointly transferring inter-model and intra-model labels for an effective image classification model.', 'Besides, Elhoseiny et al.', 'To be more general, Joyce et al.', 'Results show that there exists a significant improvement on gaze estimation and hand pose estimation using synthetic images.', '(2018b) trained a hallucinator via meta learning to generate additional examples and provide significant gains for low-shot learning.', 'This should be one of the next important focuses in AI research.', '(2016) firstly applied this mechanism into SSL.', 'Dual learning is firstly propose by (He et al., 2016a) in neural machine translation.', '(2015) firstly noticed this problem in zero-shot learning.']\n",
      "\n",
      "max score: 0.47692307215502966\n",
      "******************************\n",
      "\n",
      "Pathologic analysis of surgical excision specimens for breast carcinoma is important to evaluate the completeness of surgical excision and has implications for future treatment. This analysis is performed manually by pathologists reviewing histologic slides prepared from formalin-fixed tissue. In this paper, we present Deep Multi-Magnification Network trained by partial annotation for automated multi-class tissue segmentation by a set of patches from multiple magnifications in digitized whole slide images. Our proposed architecture with multi-encoder, multi-decoder, and multi-concatenation outperforms other single and multi-magnification-based architectures by achieving the highest mean intersection-over-union, and can be used to facilitate pathologists' assessments of breast cancer.\n",
      "\n",
      "['More recently, Deep Multi-Magnification Network (DMMN) was introduced for multi-class tissue segmentation of histopathology images by looking at patches in multiple magnifications and has shown outstanding segmentation performance in breast cancer [12].', 'It is necessary to manually label osteosarcoma whole slide images (WSIs) to supervise a segmentation convolutional neural network (CNN) for automated treatment response assessment.', 'Currently, the ratio of tumor necrosis is manually estimated by pathologists by microscopic review of multiple glass slides from resected specimens.', 'In this paper, we propose Deep Interactive Learning (DIaL) by integrating the concept of interactive learning into deep learning framework for multi-class tissue segmentation of histopathology images and treatment response assessment for osteosarcoma.', 'We used 13 cases for training and the other 42 cases for testing.', 'These challenging features can be added into the training set through Deep Interactive Learning (DIaL) by repeating training/finetuning, segmentation, and correction.', 'An example of exhaustive annotation and annotation with DIaL is shown in Figure 3.', 'Our block diagram is shown in Figure 1.']\n",
      "\n",
      "max score: 0.44559585008671376\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than 50 years ago Bongard introduced 100 visual concept learning problems as a testbed for intelligent vision systems. These problems are now known as Bongard problems. Although they are well known in the cognitive science and AI communities only moderate progress has been made towards building systems that can solve a substantial subset of them. In the system presented here, visual features are extracted through image processing and then translated into a symbolic visual vocabulary. We introduce a formal language that allows representing complex visual concepts based on this vocabulary. Using this language and Bayesian inference, complex visual concepts can be induced from the examples that are provided in each Bongard problem. Contrary to other concept learning problems the examples from which concepts are induced are not random in Bongard problems, instead they are carefully chosen to communicate the concept, hence requiring pragmatic reasoning. Taking pragmatic reasoning into account we find good agreement between the concepts with high posterior probability and the solutions formulated by Bongard himself. While this approach is far from solving all Bongard problems, it solves the biggest fraction yet.\n",
      "\n",
      "['As the properties of context-dependent perception and analogy-making perception are not strongly presented in these free-form problems, concept learning on this subset has a closer resemblance to standard few-shot visual recognition problems [4, 32].', 'Several attempts have been made in tackling the BPs with classic AI tools [12, 13], but to date we have yet to see a method capable of solving a substantial portion of the problem set.', 'A distinctive feature of the original BPs is that a visual concept can be implicitly and concisely communicated by a comparison between two sets of image examples.', 'We developed the BONGARD-LOGO benchmark that shares the same purposes as the original BPs for human-level visual concept learning and reasoning.', 'In RPMs, the relational concepts come from a small set of five relations [17].', 'For example, [13] has relied on manually specified rules that can only solve a subset of 39 carefully selected BPs, making it infeasible for our benchmark which consists of tens of thousands of problems.', 'Prior attempts on tackling the orginal BPs with symbolic reasoning [12] and program induction [13] have also been far away from solving BONGARD-LOGO.', 'We introduced a new visual cognition benchmark that emphasizes concept learning and reasoning.', 'Therefore, they have been long known in the AI research community as an inspirational challenge for visual cognition.', 'The space of all possible combinations makes the shape vocabulary size infinite.', 'Due to the increased challenge of identifying the abstract concept, we randomly sample 20 different problems for each abstract shape concept.']\n",
      "\n",
      "max score: 0.3920265732166312\n",
      "******************************\n",
      "\n",
      "The query suggestion or auto-completion mechanisms help users to type less while interacting with a search engine. A basic approach that ranks suggestions according to their frequency in the query logs is suboptimal. Firstly, many candidate queries with the same prefix can be removed as redundant. Secondly, the suggestions can also be personalised based on the user's context. These two directions to improve the aforementioned mechanisms' quality can be in opposition: while the latter aims to promote suggestions that address search intents that a user is likely to have, the former aims to diversify the suggestions to cover as many intents as possible. We introduce a contextualisation framework that utilises a short-term context using the user's behaviour within the current search session, such as the previous query, the documents examined, and the candidate query suggestions that the user has discarded. This short-term context is used to contextualise and diversify the ranking of query suggestions, by modelling the user's information need as a mixture of intent-specific user models. The evaluation is performed offline on a set of approximately 1.0M test user sessions. Our results suggest that the proposed approach significantly improves query suggestions compared to the baseline approach.\n",
      "\n",
      "['Other techniques use query clustering to similar intent classes or hierarchical models of intents [30, 54] and suggest a diverse set of queries using models that utilize a short-term context using the user’s behaviour within the current search session, such as the previous query, the documents examined, and the candidate query suggestions that the user has discarded [63], the page context that the user has browsed [29], or the whole search session [92].', 'The approach is based on probabilistic modeling, and aims to preserve keywords with similar characterizations by giving similar angles to similar keywords.', 'Studies show that both behavior inside one session and historic behavior over sessions can be used in combination or isolation to improve search results [19].', 'These findings suggest that visualized models can provide the users affordances, not only to direct their search but also to make sense of the information potentially available.', 'The search engine automatically logged the timestamp and the action performed by the user.', 'We recruited 24 participants from two universities.', 'A screenshot of the IntentRadar system is shown in Figures 1 and 2.', 'The findings from the exploratory search experiment show that interactive intent modeling significantly improves retrieval performance over the search session.', 'Both systems used the same document set.', 'Our findings can help design personalized systems that support exploratory information seeking and discovery of novel information.']\n",
      "\n",
      "max score: 0.48120300258239584\n",
      "******************************\n",
      "\n",
      "This paper proposes OCR++, an open-source framework designed for a variety of information extraction tasks from scholarly articles including metadata (title, author names, affiliation and e-mail), structure (section headings and body text, table and figure headings, URLs and footnotes) and bibliography (citation instances and references). We analyze a diverse set of scientific articles written in English language to understand generic writing patterns and formulate rules to develop this hybrid framework. Extensive evaluations show that the proposed framework outperforms the existing state-of-the-art tools with huge margin in structural information extraction along with improved performance in metadata and bibliography extraction tasks, both in terms of accuracy (around 50% improvement) and processing time (around 52% improvement). A user experience study conducted with the help of 30 researchers reveals that the researchers found this system to be very helpful. As an additional objective, we discuss two novel use cases including automatically extracting links to public datasets from the proceedings, which would further accelerate the advancement in digital libraries. The result of the framework can be exported as a whole into structured TEI-encoded documents. Our framework is accessible online at http://cnergres.iitkgp.ac.in/OCR++/home/.\n",
      "\n",
      "['[32] proposed an open-source system for a variety of scholarly paper knowledge extraction tasks, including metadata (title, name of author, affiliation and email), structure (headings of section and body text, headings of table and figure, URLs and footnotes), and bibliography (citation instances and references).', 'In this paper, therefore, we present the design and implementation of a system along with the required components that should be able to automatically and regularly fetch digital brochures from online sources to be then subjected to further processing.', 'The second step consists in converting the PDF documents into TXT format.', 'Consequently, automating the process of extracting useful information from brochures is desired.', 'This framework made use of supervised and unsupervised machine learning techniques.', 'Our prototype has been tested using brochures from two different supermarkets demonstrating a good performance.', 'As shown in Fig.', 'We expect that such a system would allow decision makers to have quick access to information that can be used to better formulate marketing and sales strategies.', '(1), we obtained a good accuracy of 85%.']\n",
      "\n",
      "max score: 0.4420289805096094\n",
      "******************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(malnis_dataset)\n",
    "\n",
    "for q, d in zip(tqdm(data[\"query\"]), data.text):\n",
    "    s, m = malnis_dataset.find_summary(q, d)\n",
    "    print(q)\n",
    "    print()\n",
    "    print(s)\n",
    "    print()\n",
    "    print(\"max score:\", m)\n",
    "    print(\"*\" * 30)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a499e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
