{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e8ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import malnis\n",
    "from malnis import show\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c2c23c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_id</th>\n",
       "      <th>text</th>\n",
       "      <th>query</th>\n",
       "      <th>cited</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>Our approach is mainly based on the BERT langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "      <td>BERT [2] is a self-supervised approach for pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td>Recently, some variants [4, 12] of BERT langua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>With the capability of modeling bidirectional ...</td>\n",
       "      <td>Recently, some variants [4, 12] of BERT langua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>Neural sequence-to-sequence models have provid...</td>\n",
       "      <td>The proposed model is based on the pointer-gen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          reference_id                                               text  \\\n",
       "paper_id                                                                    \n",
       "0                    1  KEYWORDS cascade ranking, pre-trained language...   \n",
       "0                    2  KEYWORDS cascade ranking, pre-trained language...   \n",
       "0                   13  KEYWORDS cascade ranking, pre-trained language...   \n",
       "0                   20  KEYWORDS cascade ranking, pre-trained language...   \n",
       "0                   26  KEYWORDS cascade ranking, pre-trained language...   \n",
       "\n",
       "                                                      query  \\\n",
       "paper_id                                                      \n",
       "0         We introduce a new language representation mod...   \n",
       "0         The dominant sequence transduction models are ...   \n",
       "0         Language model pretraining has led to signific...   \n",
       "0         With the capability of modeling bidirectional ...   \n",
       "0         Neural sequence-to-sequence models have provid...   \n",
       "\n",
       "                                                      cited  \n",
       "paper_id                                                     \n",
       "0         Our approach is mainly based on the BERT langu...  \n",
       "0         BERT [2] is a self-supervised approach for pre...  \n",
       "0         Recently, some variants [4, 12] of BERT langua...  \n",
       "0         Recently, some variants [4, 12] of BERT langua...  \n",
       "0         The proposed model is based on the pointer-gen...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/full_examples.csv\", index_col = 0).head()\n",
    "show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49240384",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d631de5b39439bac173cef6282e766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
      "\n",
      "['Our approach is mainly based on the BERT language model [2], which is a state-of-the-art model in various natural language understanding tasks.BERT [2] is a self-supervised approach for pre-training a deep transformer encoder [8], before fine-tuning it for a particular downstream task.', 'Then, we finetune the pre-trained BERT model on labeled query-passage data with a point-wise ranking strategy.', 'Then, we design a new pre-trained BERT language model for re-ranking, by enriched with more fine-grained sentence structure information.', 'One of the key points of BERT lies in how to design more appropriate pre-training objectives with the unlabeled text.', 'Thenwe build index on the collection of expanded passages, and use a simple and effective BM25 method to retrieve the top-k candidate passages.', 'The details of task construction, evaluation methods and result analysis can be found in the overview paper of the TREC 2019 Deep Learning Track [1].', 'The experiment results show that our method can obtain superior performance compared with other competitive submission runs on both the passage ranking and document ranking tasks.']\n",
      "\n",
      "max score: 0.06535947215002814\n",
      "******************************\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "\n",
      "['BERT [2] is a self-supervised approach for pre-training a deep transformer encoder [8], before fine-tuning it for a particular downstream task.', 'Prior to indexing, we propose a query generation method for document expansion based on the pointer-generator model, where each document is expanded with a set of generated queries for improved retrieval.', 'At each decoding step, the attention distributions et i of encoder states and the context vector ct are given as: et i = v T tanh(Whhi +Wsst ) (1) ct = N∑ i=1 αetihi , α e t = so f tmax (et ) (2) where v,Wh ,Ws are trainable parameters, st is the decoder hidden state at time step t . Traditional attention mechanism just calculates the attention distribution of the encoder hidden states but ignores the decoder hidden states.', 'https://doi.org/10.1145/nnnnnnn.nnnnnnn The Deep Learning Track is a new track first run in TREC 2019, which aims at studying information retrieval in a large training data regime.', 'As for document expansion, for each passage p = {x1, · · · ,xN } ∈ P, we aim to predict a set of queries Qдen = {qдen1 , · · · ,q дen L } for which that passage will be relevant, where qдen = {q1, · · · ,qM } andM is the total number of generated query words.We first extract a total collection of query-relevant passage pairs from the labeled training corpus, and use them to train an encoder-decoder network for query generation from the passage.', 'The full corpus is 3,213,835 documents and the training set has 367,013 queries.', '• Recalling less passages/documents in retriever stage gives the best result in terms of NDCG@10, but decreases the AP and Recall@100 metrics slightly.', 'The BERT ranker predicts the relevance of each passage independently, and the document score is calculated as the maximum score of all the passages within the document.', 'The experiment results show that our method can obtain superior performance compared with other competitive submission runs on both the passage ranking and document ranking tasks.', 'Recently, some variants [4, 12] of BERT language model found that the NSP task may be harmful to learn a effective language model compared with MLM task.']\n",
      "\n",
      "max score: 0.06336633226185699\n",
      "******************************\n",
      "\n",
      "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.\n",
      "\n",
      "['Recently, some variants [4, 12] of BERT language model found that the NSP task may be harmful to learn a effective language model compared with MLM task.', 'At decoding time, the decoder will sequentially generate query words by attending on the passage hidden states.', 'Therefore, during the retriever and re-ranker stages, we both add the title to the beginning of every splitted passage to provide context.', 'The probability score is accumulated with different BERT re-rankers, and the sum of the probability scores is used to calculate the final rankings.', 'To test the effectiveness of different components in our method, we submit the first three full ranking runs with different settings, i.e.', 'https://doi.org/10.1145/nnnnnnn.nnnnnnn The Deep Learning Track is a new track first run in TREC 2019, which aims at studying information retrieval in a large training data regime.', 'In both the BERT pretraining and fine-tuning, we use the 12-layer BERT base architecture (L = 12,H = 768,A = 12), the max sequence length is set at 384.']\n",
      "\n",
      "max score: 0.047781565073559885\n",
      "******************************\n",
      "\n",
      "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.\n",
      "\n",
      "['Recently, some variants [4, 12] of BERT language model found that the NSP task may be harmful to learn a effective language model compared with MLM task.', 'It consists of two tasks: passage ranking and document ranking.', 'Our approach is mainly based on the BERT language model [2], which is a state-of-the-art model in various natural language understanding tasks.', 'Firstly, we propose to leverage a sequence-to-sequence generation method to conduct document expansion, which helps to retain a higher recall of the candidate passages from the whole passage collection.', 'Both tasks use a large human-generated set of training labels, from the MS-MARCO 1 dataset.', 'In the second stage, we further leverage the state-of-the-art BERT languagemodel to re-rank the candidate passages/documents.', 'With the top-k candidate passages from retriever stage, we further take the BERT language model to re-rank the candidate passages for final ranking.']\n",
      "\n",
      "max score: 0.07874015248062528\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.\n",
      "\n",
      "['The proposed model is based on the pointer-generator model [7], which is widely used in abstractive text summarization.Following [7], we also use a soft switch to choose between generating a word from the vocabulary or copying a word from the input sequence, and calculate the final probability distribution over the extended vocabulary as:where pдen is a soft switch used in [7].', 'To better distinguish among generated words in the query, we propose a novel Attention Over Attention (AOA) mechanism to consider both the attention distributions of encoder hidden states and the previous decoder hidden states.', 'With the top-k candidate passages from retriever stage, we further take the BERT language model to re-rank the candidate passages for final ranking.', 'Therefore, we adopt a simple passage-level approach for document retrieval.', 'The MLM task is kept the same as in the original BERT.', 'Copyrights for components of this work owned by others than ACM must be honored.', 'For the BM25 search engine, we use the off-the-shelf Anserini open-source IR toolkit [11].', 'In this paper, we propose an effective cascade ranking framework for ad-hoc passage and document retrieval.', 'Our approach is mainly based on the BERT language model [2], which is a state-of-the-art model in various natural language understanding tasks.']\n",
      "\n",
      "max score: 0.09302325105800456\n",
      "******************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(malnis)\n",
    "\n",
    "for q, d, c in zip(tqdm(data[\"query\"]), data.text, data.cited):\n",
    "    s, m, d = malnis.find_summary(\n",
    "        q, \n",
    "        d, \n",
    "        metric = \"rouge-2\", \n",
    "        component = \"f\", \n",
    "        starting_summary = c\n",
    "    )\n",
    "    print(q)\n",
    "    print()\n",
    "    print(s)\n",
    "    print()\n",
    "    print(\"max score:\", m)\n",
    "#     print(d)\n",
    "    print(\"*\" * 30)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a499e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
