{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae054c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarobyte/envs/malnis/lib/python3.8/site-packages/huggingface_hub/snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from malnis import show\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.sparse as sp\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, auc, roc_auc_score, average_precision_score, PrecisionRecallDisplay\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57351439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jarobyte/malnis_dataset/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d48d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>rl</th>\n",
       "      <th>sentences</th>\n",
       "      <th>relevance</th>\n",
       "      <th>n_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For many types of machine learning algorithms,...</td>\n",
       "      <td>field in computer science, and health informat...</td>\n",
       "      <td>[While the techniques for neural networks are ...</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>[field in computer science, and health informa...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We consider the problem of learning good traje...</td>\n",
       "      <td>field in computer science, and health informat...</td>\n",
       "      <td>[[71] considered the problem of learning good ...</td>\n",
       "      <td>0.646766</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.636816</td>\n",
       "      <td>[field in computer science, and health informa...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HDBSCAN*, a state-of-the-art density-based hie...</td>\n",
       "      <td>PVLDB Reference Format: Antonio Cavalcante Ara...</td>\n",
       "      <td>[In [6], the authors proposed RNG-HDBSCAN*, a ...</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.350365</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>[PVLDB Reference Format: Antonio Cavalcante Ar...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This work investigates continual learning of t...</td>\n",
       "      <td>Fully automatic deep learning has become the s...</td>\n",
       "      <td>[Baweja et al. (2018) investigate continual le...</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.317690</td>\n",
       "      <td>0.442308</td>\n",
       "      <td>[Fully automatic deep learning has become the ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Query relevance ranking and sentence saliency ...</td>\n",
       "      <td>Keywords: Query-focused summarization · Extrac...</td>\n",
       "      <td>[There exist approaches utilizing attention me...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>[Keywords: Query-focused summarization · Extra...</td>\n",
       "      <td>[False, False, True, False, False, False, Fals...</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The key idea behind active learning is that a ...</td>\n",
       "      <td>I. INTRODUCTION Deep Neural Networks (DNNs) tr...</td>\n",
       "      <td>[The key idea behind active learning is that a...</td>\n",
       "      <td>0.368664</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>0.368664</td>\n",
       "      <td>[I., INTRODUCTION Deep Neural Networks (DNNs) ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When AI systems interact with humans in the lo...</td>\n",
       "      <td>22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 2...</td>\n",
       "      <td>[• Approaches: Past work on explanations prima...</td>\n",
       "      <td>0.330827</td>\n",
       "      <td>0.278075</td>\n",
       "      <td>0.330827</td>\n",
       "      <td>[22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pre-training techniques have been verified suc...</td>\n",
       "      <td>Visually-rich Document Understanding (VrDU) ai...</td>\n",
       "      <td>[To this end, the second direction relies on t...</td>\n",
       "      <td>0.459259</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.437037</td>\n",
       "      <td>[Visually-rich Document Understanding (VrDU) a...</td>\n",
       "      <td>[False, False, True, False, False, False, Fals...</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We rigorously evaluate three state-of-the-art ...</td>\n",
       "      <td>The Transformer (Vaswani et al., 2017) has bec...</td>\n",
       "      <td>[This agrees with the observations made in wor...</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>0.264957</td>\n",
       "      <td>0.395604</td>\n",
       "      <td>[The Transformer (Vaswani et al., 2017) has be...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We introduce a novel type of text representati...</td>\n",
       "      <td>We experiment on real world invoice and resume...</td>\n",
       "      <td>[Chargrid [14] models the problem by encoding ...</td>\n",
       "      <td>0.423358</td>\n",
       "      <td>0.258427</td>\n",
       "      <td>0.423358</td>\n",
       "      <td>[We experiment on real world invoice and resum...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We present RACE, a new dataset for benchmark e...</td>\n",
       "      <td>Unsupervised representation learning has been ...</td>\n",
       "      <td>[The RACE dataset [17] contains near 100K ques...</td>\n",
       "      <td>0.490323</td>\n",
       "      <td>0.251208</td>\n",
       "      <td>0.464516</td>\n",
       "      <td>[Unsupervised representation learning has been...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>Proceedings of the 57th Annual Meeting of the ...</td>\n",
       "      <td>[Firstly, to better capture sentential meaning...</td>\n",
       "      <td>0.420635</td>\n",
       "      <td>0.244648</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>[Proceedings of the 57th Annual Meeting of the...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Recurrent neural networks, and in particular l...</td>\n",
       "      <td>Index Terms—Deep learning, visual analytics, i...</td>\n",
       "      <td>[In response to this, many visual analytics sy...</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.238372</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>[Index Terms—Deep learning, visual analytics, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Several large cloze-style context-question-ans...</td>\n",
       "      <td>Building intelligent agents with machine readi...</td>\n",
       "      <td>[• Attention Sum Reader (AS Reader): AS Reader...</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.234146</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>[Building intelligent agents with machine read...</td>\n",
       "      <td>[False, False, False, False, False, False, Tru...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Several large cloze-style context-question-ans...</td>\n",
       "      <td>Building intelligent agents with machine readi...</td>\n",
       "      <td>[• Attention Sum Reader (AS Reader): AS Reader...</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.234146</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>[Building intelligent agents with machine read...</td>\n",
       "      <td>[False, False, False, False, False, False, Tru...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The key idea behind active learning is that a ...</td>\n",
       "      <td>CCS Concepts • Computing methodologies➝Maching...</td>\n",
       "      <td>[It’s well known that the key idea lying behin...</td>\n",
       "      <td>0.434343</td>\n",
       "      <td>0.231047</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>[CCS Concepts • Computing methodologies➝Machin...</td>\n",
       "      <td>[False, False, False, False, True, False, Fals...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The key idea behind active learning is that a ...</td>\n",
       "      <td>We propose a mixed-initiative active learning ...</td>\n",
       "      <td>[Settles [32] writes that “Active learning sys...</td>\n",
       "      <td>0.382022</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.382022</td>\n",
       "      <td>[We propose a mixed-initiative active learning...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>There has been a recent resurgence in the area...</td>\n",
       "      <td>Index Terms—Deep learning, visual analytics, i...</td>\n",
       "      <td>[referring to machine learning models [60], [6...</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.220472</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>[Index Terms—Deep learning, visual analytics, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Recently, pre-trained models have achieved sta...</td>\n",
       "      <td>Text classification (Korde and Mahender, 2012;...</td>\n",
       "      <td>[0 (Sun et al., 2019) proposed a continual pre...</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.218045</td>\n",
       "      <td>0.394089</td>\n",
       "      <td>[Text classification (Korde and Mahender, 2012...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ROUGE stands for Recall-Oriented Understudy fo...</td>\n",
       "      <td>ar X iv :1 70 7. 02 26 8v 3 [ cs .C L In recen...</td>\n",
       "      <td>[Lin [36] introduced a set ofmetrics called Re...</td>\n",
       "      <td>0.319527</td>\n",
       "      <td>0.217195</td>\n",
       "      <td>0.319527</td>\n",
       "      <td>[ar X iv :1 70 7., 02 26 8v 3 [ cs .C L In rec...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   For many types of machine learning algorithms,...   \n",
       "1   We consider the problem of learning good traje...   \n",
       "2   HDBSCAN*, a state-of-the-art density-based hie...   \n",
       "3   This work investigates continual learning of t...   \n",
       "4   Query relevance ranking and sentence saliency ...   \n",
       "5   The key idea behind active learning is that a ...   \n",
       "6   When AI systems interact with humans in the lo...   \n",
       "7   Pre-training techniques have been verified suc...   \n",
       "8   We rigorously evaluate three state-of-the-art ...   \n",
       "9   We introduce a novel type of text representati...   \n",
       "10  We present RACE, a new dataset for benchmark e...   \n",
       "11  We introduce a new language representation mod...   \n",
       "12  Recurrent neural networks, and in particular l...   \n",
       "13  Several large cloze-style context-question-ans...   \n",
       "14  Several large cloze-style context-question-ans...   \n",
       "15  The key idea behind active learning is that a ...   \n",
       "16  The key idea behind active learning is that a ...   \n",
       "17  There has been a recent resurgence in the area...   \n",
       "18  Recently, pre-trained models have achieved sta...   \n",
       "19  ROUGE stands for Recall-Oriented Understudy fo...   \n",
       "\n",
       "                                             document  \\\n",
       "0   field in computer science, and health informat...   \n",
       "1   field in computer science, and health informat...   \n",
       "2   PVLDB Reference Format: Antonio Cavalcante Ara...   \n",
       "3   Fully automatic deep learning has become the s...   \n",
       "4   Keywords: Query-focused summarization · Extrac...   \n",
       "5   I. INTRODUCTION Deep Neural Networks (DNNs) tr...   \n",
       "6   22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 2...   \n",
       "7   Visually-rich Document Understanding (VrDU) ai...   \n",
       "8   The Transformer (Vaswani et al., 2017) has bec...   \n",
       "9   We experiment on real world invoice and resume...   \n",
       "10  Unsupervised representation learning has been ...   \n",
       "11  Proceedings of the 57th Annual Meeting of the ...   \n",
       "12  Index Terms—Deep learning, visual analytics, i...   \n",
       "13  Building intelligent agents with machine readi...   \n",
       "14  Building intelligent agents with machine readi...   \n",
       "15  CCS Concepts • Computing methodologies➝Maching...   \n",
       "16  We propose a mixed-initiative active learning ...   \n",
       "17  Index Terms—Deep learning, visual analytics, i...   \n",
       "18  Text classification (Korde and Mahender, 2012;...   \n",
       "19  ar X iv :1 70 7. 02 26 8v 3 [ cs .C L In recen...   \n",
       "\n",
       "                                              summary        r1        r2  \\\n",
       "0   [While the techniques for neural networks are ...  0.594595  0.485714   \n",
       "1   [[71] considered the problem of learning good ...  0.646766  0.478571   \n",
       "2   [In [6], the authors proposed RNG-HDBSCAN*, a ...  0.539062  0.350365   \n",
       "3   [Baweja et al. (2018) investigate continual le...  0.480769  0.317690   \n",
       "4   [There exist approaches utilizing attention me...  0.500000  0.293333   \n",
       "5   [The key idea behind active learning is that a...  0.368664  0.282828   \n",
       "6   [• Approaches: Past work on explanations prima...  0.330827  0.278075   \n",
       "7   [To this end, the second direction relies on t...  0.459259  0.265000   \n",
       "8   [This agrees with the observations made in wor...  0.406593  0.264957   \n",
       "9   [Chargrid [14] models the problem by encoding ...  0.423358  0.258427   \n",
       "10  [The RACE dataset [17] contains near 100K ques...  0.490323  0.251208   \n",
       "11  [Firstly, to better capture sentential meaning...  0.420635  0.244648   \n",
       "12  [In response to this, many visual analytics sy...  0.425000  0.238372   \n",
       "13  [• Attention Sum Reader (AS Reader): AS Reader...  0.342105  0.234146   \n",
       "14  [• Attention Sum Reader (AS Reader): AS Reader...  0.342105  0.234146   \n",
       "15  [It’s well known that the key idea lying behin...  0.434343  0.231047   \n",
       "16  [Settles [32] writes that “Active learning sys...  0.382022  0.224900   \n",
       "17  [referring to machine learning models [60], [6...  0.431818  0.220472   \n",
       "18  [0 (Sun et al., 2019) proposed a continual pre...  0.413793  0.218045   \n",
       "19  [Lin [36] introduced a set ofmetrics called Re...  0.319527  0.217195   \n",
       "\n",
       "          rl                                          sentences  \\\n",
       "0   0.594595  [field in computer science, and health informa...   \n",
       "1   0.636816  [field in computer science, and health informa...   \n",
       "2   0.507812  [PVLDB Reference Format: Antonio Cavalcante Ar...   \n",
       "3   0.442308  [Fully automatic deep learning has become the ...   \n",
       "4   0.490566  [Keywords: Query-focused summarization · Extra...   \n",
       "5   0.368664  [I., INTRODUCTION Deep Neural Networks (DNNs) ...   \n",
       "6   0.330827  [22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 ...   \n",
       "7   0.437037  [Visually-rich Document Understanding (VrDU) a...   \n",
       "8   0.395604  [The Transformer (Vaswani et al., 2017) has be...   \n",
       "9   0.423358  [We experiment on real world invoice and resum...   \n",
       "10  0.464516  [Unsupervised representation learning has been...   \n",
       "11  0.412698  [Proceedings of the 57th Annual Meeting of the...   \n",
       "12  0.383333  [Index Terms—Deep learning, visual analytics, ...   \n",
       "13  0.342105  [Building intelligent agents with machine read...   \n",
       "14  0.342105  [Building intelligent agents with machine read...   \n",
       "15  0.404040  [CCS Concepts • Computing methodologies➝Machin...   \n",
       "16  0.382022  [We propose a mixed-initiative active learning...   \n",
       "17  0.424242  [Index Terms—Deep learning, visual analytics, ...   \n",
       "18  0.394089  [Text classification (Korde and Mahender, 2012...   \n",
       "19  0.319527  [ar X iv :1 70 7., 02 26 8v 3 [ cs .C L In rec...   \n",
       "\n",
       "                                            relevance  n_sentences  \n",
       "0   [False, False, False, False, False, False, Fal...          256  \n",
       "1   [False, False, False, False, False, False, Fal...          256  \n",
       "2   [False, False, False, False, False, False, Fal...          124  \n",
       "3   [False, False, False, False, False, False, Fal...          449  \n",
       "4   [False, False, True, False, False, False, Fals...          266  \n",
       "5   [False, False, False, False, False, False, Fal...          238  \n",
       "6   [False, False, False, False, False, False, Fal...          166  \n",
       "7   [False, False, True, False, False, False, Fals...          252  \n",
       "8   [False, False, False, False, False, False, Fal...          233  \n",
       "9   [False, False, False, False, False, False, Fal...          301  \n",
       "10  [False, False, False, False, False, False, Fal...          263  \n",
       "11  [False, False, False, False, False, False, Fal...          186  \n",
       "12  [False, False, False, False, False, False, Fal...          497  \n",
       "13  [False, False, False, False, False, False, Tru...          185  \n",
       "14  [False, False, False, False, False, False, Tru...          185  \n",
       "15  [False, False, False, False, True, False, Fals...          165  \n",
       "16  [False, False, False, False, False, False, Fal...          237  \n",
       "17  [False, False, False, False, False, False, Fal...          497  \n",
       "18  [False, False, False, False, False, False, Fal...          302  \n",
       "19  [False, False, False, False, False, False, Fal...          283  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of sentences in each paper goes from 59 to 4,447. I truncate to 512.\n",
    "\n",
    "data = pd.read_pickle(\"../data/labels.pkl\")\\\n",
    ".assign(n_sentences = lambda df: df.sentences.map(len))\\\n",
    ".query(\"n_sentences <= 512\")\\\n",
    ".reset_index(drop = True)\n",
    "\n",
    "show(data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a3d01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.r2.plot.hist(log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8e3bb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications to computing similarity between authors and entropy of author output.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"query\"][36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e05fa16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The dynamic mixture model (DMM) [36] considers a single dynamic sequence of documents, which corresponds to a single topic over time.',\n",
       " 'We sample the topic assignment z for each word-pair instead of each independent word.',\n",
       " 'UCT captures a user’s interests at time t as θt,u, that is, as a distribution over mixtures of topics.',\n",
       " '(6) Inference is intractable in this model.',\n",
       " 'This is a traditional clustering algorithm [16].',\n",
       " 'Latent Dirichlet Allocation (LDA).',\n",
       " 'We evaluate the performance with the above metrics at each time period, and report the mean of the evaluation results.',\n",
       " 'The author-topic model for authors and documents.',\n",
       " 'Dynamic mixture models for multiple time-series.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s, l in zip(data.sentences[36], data.relevance[36]) if l == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d082009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data.relevance[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f45f2977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.relevance[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d57f0b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer(\n",
    "    \"sbert\"\n",
    "#     'all-MiniLM-L6-v2', \n",
    "#     cache_folder = \"../assets\"\n",
    "#     \"../cache/huggingface/transformers/\"\n",
    "#     cache_folder = \"../cache/huggingface/transformers\"\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ff1df69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 384)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings = model.encode(data[\"query\"])\n",
    "query_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a69cbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a23e467d4c54ad0b0b3f58ec7cac4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = query_embeddings.shape[1]\n",
    "sentence_embeddings = [\n",
    "    model.encode(l)#.toarray() \n",
    "    for l in tqdm(data.sentences)\n",
    "]\n",
    "all([l.shape[1] == dims for l in sentence_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07ff9f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings.shape[0] == len(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed72a578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220a16668a114efea1c6d5b82dde02d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "73124"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(l) for l in tqdm(data.sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eacf1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = [\n",
    "# #     ((sp.csr_matrix(np.ones([l.shape[0],1])) * q) - l).power(2)\n",
    "# #     (q - l)**2\n",
    "#     np.concatenate([np.tile(q, (l.shape[0], 1)), l], axis = 1)\n",
    "#     for q, l in zip(tqdm(query_embeddings), sentence_embeddings)\n",
    "# ]\n",
    "# len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fab9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x.shape for x in train[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a39b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series([x.shape[0] for x in train]).describe(percentiles = np.linspace(0, 1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cad51f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.sentences.map(len).plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45811370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = [torch.tensor(x) for x in train]\n",
    "# all([x.shape[0] <= 512 for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6342443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.nn.utils.rnn.pad_sequence(X, batch_first = False)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f122e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.concatenate(train)#.toarray().T#.squeeze()\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d616105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(X, \"X__lstm_sbert_concat.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65ed2afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([275, 504, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.load(\"X__lstm_sbert_concat.npy\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0ee380f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75119cd5f5a34409982a610a6f3e65c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([73124])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.tensor([y for l in tqdm(data.relevance) for y in l])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0a3748d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([275, 504])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.nn.utils.rnn.pad_sequence([torch.tensor(x) for x in data.relevance], batch_first = True)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "568c0951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2470)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f80e15b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = LogisticRegression(\n",
    "#     C = 10**-2\n",
    "# )\n",
    "# model.fit(X, Y)\n",
    "\n",
    "# predictions = model.predict_proba(X)\n",
    "\n",
    "# # predictions.sum()\n",
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b31159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = LogisticRegression(\n",
    "#     C = 10**-2\n",
    "# )\n",
    "# model.fit(X, Y)\n",
    "\n",
    "# predictions = model.predict_proba(X)\n",
    "\n",
    "# # predictions.sum()\n",
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d50be69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = SVC(\n",
    "# #     C = 10**-2\n",
    "# )\n",
    "# model.fit(X, Y)\n",
    "\n",
    "# predictions = model.predict_proba(X)\n",
    "\n",
    "# # predictions.sum()\n",
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0426c05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = MLPClassifier(\n",
    "# #     C = 10**-2\n",
    "#     verbose = True,\n",
    "# #     early_stopping = True\n",
    "# )\n",
    "# model.fit(X, Y)\n",
    "\n",
    "# predictions = model.predict_proba(X)\n",
    "\n",
    "# # predictions.sum()\n",
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35f8b580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train torch.Size([206, 504, 768])\n",
      "X_test torch.Size([69, 504, 768])\n",
      "Y_train torch.Size([206, 504])\n",
      "Y_test torch.Size([69, 504])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"Y_train\", Y_train.shape)\n",
    "print(\"Y_test\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ad173cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1790), tensor(680))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.sum(), Y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acdb2183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model = MLPClassifier(\n",
    "# #     C = 10**-2\n",
    "#     verbose = True,\n",
    "# #     early_stopping = True\n",
    "# )\n",
    "# model.fit(X_train, Y_train)\n",
    "\n",
    "# predictions = model.predict_proba(X_test)\n",
    "\n",
    "# # predictions.sum()\n",
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9485282e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 504, 768])\n",
      "(tensor([[[-6.1515e-02, -1.1650e-03,  5.6224e-03,  ...,  3.7993e-02,\n",
      "           1.6307e-02,  6.7649e-02],\n",
      "         [-6.3048e-02,  3.2548e-03,  1.3215e-03,  ...,  3.9316e-02,\n",
      "           5.2120e-03,  6.5406e-02],\n",
      "         [-6.5201e-02,  2.9311e-02,  5.0081e-03,  ...,  1.9785e-02,\n",
      "           4.0859e-02,  4.1452e-02],\n",
      "         ...,\n",
      "         [-3.5228e-02, -1.0815e-02,  7.1415e-03,  ...,  5.6132e-02,\n",
      "           4.4949e-02,  2.6575e-02],\n",
      "         [-3.5228e-02, -1.0815e-02,  7.1415e-03,  ...,  5.6132e-02,\n",
      "           4.4949e-02,  2.6575e-02],\n",
      "         [-3.5228e-02, -1.0815e-02,  7.1415e-03,  ...,  5.6132e-02,\n",
      "           4.4949e-02,  2.6575e-02]],\n",
      "\n",
      "        [[-1.0066e-01, -1.7871e-02,  6.7002e-06,  ...,  4.2436e-02,\n",
      "           1.6605e-02,  2.3631e-02],\n",
      "         [-1.1764e-01,  1.3606e-02,  5.5039e-04,  ...,  3.8308e-02,\n",
      "           3.4052e-02,  6.6408e-02],\n",
      "         [-1.0262e-01,  3.1461e-02,  3.1650e-02,  ...,  2.7030e-02,\n",
      "           4.5487e-02,  6.6837e-02],\n",
      "         ...,\n",
      "         [-5.6298e-02, -1.5956e-02,  1.4866e-02,  ...,  5.4053e-02,\n",
      "           4.1637e-02,  2.4099e-02],\n",
      "         [-5.6298e-02, -1.5956e-02,  1.4866e-02,  ...,  5.4053e-02,\n",
      "           4.1637e-02,  2.4099e-02],\n",
      "         [-5.6298e-02, -1.5956e-02,  1.4866e-02,  ...,  5.4053e-02,\n",
      "           4.1637e-02,  2.4099e-02]],\n",
      "\n",
      "        [[-1.0678e-01, -1.5666e-02,  4.3898e-03,  ...,  4.4639e-02,\n",
      "           2.8178e-02,  1.3225e-02],\n",
      "         [-1.3897e-01, -1.6301e-02,  7.0264e-03,  ...,  1.6158e-02,\n",
      "           4.9466e-02,  3.0245e-02],\n",
      "         [-9.8791e-02,  3.3730e-03,  3.9516e-02,  ...,  3.1899e-02,\n",
      "           4.7090e-02,  2.2710e-02],\n",
      "         ...,\n",
      "         [-6.7704e-02, -1.8776e-02,  2.1169e-02,  ...,  4.9999e-02,\n",
      "           3.6746e-02,  2.0233e-02],\n",
      "         [-6.7704e-02, -1.8776e-02,  2.1169e-02,  ...,  4.9999e-02,\n",
      "           3.6746e-02,  2.0233e-02],\n",
      "         [-6.7704e-02, -1.8776e-02,  2.1169e-02,  ...,  4.9999e-02,\n",
      "           3.6746e-02,  2.0233e-02]],\n",
      "\n",
      "        [[-1.0080e-01, -2.5767e-02, -2.1082e-03,  ...,  5.1402e-02,\n",
      "           3.8279e-02, -9.9994e-03],\n",
      "         [-1.4669e-01, -1.4477e-02,  1.2143e-02,  ...,  4.4672e-02,\n",
      "           3.2893e-02, -5.1293e-03],\n",
      "         [-1.3413e-01,  2.1965e-03,  4.7575e-02,  ...,  3.0033e-02,\n",
      "           2.3578e-02, -2.3709e-03],\n",
      "         ...,\n",
      "         [-7.3445e-02, -2.0408e-02,  2.5721e-02,  ...,  4.2125e-02,\n",
      "           2.9569e-02,  1.4597e-02],\n",
      "         [-7.3445e-02, -2.0408e-02,  2.5721e-02,  ...,  4.2125e-02,\n",
      "           2.9569e-02,  1.4597e-02],\n",
      "         [-7.3445e-02, -2.0408e-02,  2.5721e-02,  ...,  4.2125e-02,\n",
      "           2.9569e-02,  1.4597e-02]],\n",
      "\n",
      "        [[-9.3615e-02, -3.8954e-02,  7.3294e-03,  ...,  3.4912e-02,\n",
      "           2.5433e-02,  8.8661e-03],\n",
      "         [-1.3724e-01, -5.5137e-02,  2.0437e-02,  ...,  3.0940e-02,\n",
      "           3.2096e-02, -1.2828e-03],\n",
      "         [-1.2137e-01, -3.2570e-02,  3.2129e-02,  ...,  5.2877e-02,\n",
      "           1.5733e-02,  1.9267e-02],\n",
      "         ...,\n",
      "         [-7.6157e-02, -2.1340e-02,  2.8775e-02,  ...,  2.7140e-02,\n",
      "           1.8672e-02,  7.2706e-03],\n",
      "         [-7.6157e-02, -2.1340e-02,  2.8775e-02,  ...,  2.7140e-02,\n",
      "           1.8672e-02,  7.2706e-03],\n",
      "         [-7.6157e-02, -2.1340e-02,  2.8775e-02,  ...,  2.7140e-02,\n",
      "           1.8672e-02,  7.2706e-03]]], grad_fn=<CatBackward0>), (tensor([[[-0.0936, -0.0390,  0.0073,  ...,  0.0449,  0.0275,  0.0090],\n",
      "         [-0.1372, -0.0551,  0.0204,  ...,  0.0437,  0.0423,  0.0251],\n",
      "         [-0.1214, -0.0326,  0.0321,  ...,  0.0473,  0.0488, -0.0051],\n",
      "         ...,\n",
      "         [-0.0762, -0.0213,  0.0288,  ...,  0.0790,  0.0246, -0.0087],\n",
      "         [-0.0762, -0.0213,  0.0288,  ...,  0.0790,  0.0246, -0.0087],\n",
      "         [-0.0762, -0.0213,  0.0288,  ...,  0.0790,  0.0246, -0.0087]],\n",
      "\n",
      "        [[ 0.1167,  0.0716,  0.1432,  ...,  0.0380,  0.0163,  0.0676],\n",
      "         [ 0.1041,  0.1042,  0.1019,  ...,  0.0393,  0.0052,  0.0654],\n",
      "         [ 0.0979,  0.0916,  0.1240,  ...,  0.0198,  0.0409,  0.0415],\n",
      "         ...,\n",
      "         [ 0.0662,  0.0396,  0.0661,  ...,  0.0561,  0.0449,  0.0266],\n",
      "         [ 0.0662,  0.0396,  0.0661,  ...,  0.0561,  0.0449,  0.0266],\n",
      "         [ 0.0662,  0.0396,  0.0661,  ...,  0.0561,  0.0449,  0.0266]]],\n",
      "       grad_fn=<StackBackward0>), tensor([[[-0.2013, -0.0757,  0.0161,  ...,  0.0957,  0.0577,  0.0188],\n",
      "         [-0.3089, -0.1041,  0.0460,  ...,  0.0921,  0.0866,  0.0511],\n",
      "         [-0.2613, -0.0641,  0.0709,  ...,  0.0947,  0.0979, -0.0105],\n",
      "         ...,\n",
      "         [-0.1546, -0.0407,  0.0626,  ...,  0.1529,  0.0521, -0.0178],\n",
      "         [-0.1546, -0.0407,  0.0626,  ...,  0.1529,  0.0521, -0.0178],\n",
      "         [-0.1546, -0.0407,  0.0626,  ...,  0.1529,  0.0521, -0.0178]],\n",
      "\n",
      "        [[ 0.2532,  0.1460,  0.3045,  ...,  0.0751,  0.0341,  0.1274],\n",
      "         [ 0.2257,  0.2116,  0.2073,  ...,  0.0790,  0.0110,  0.1270],\n",
      "         [ 0.2153,  0.1795,  0.2620,  ...,  0.0426,  0.0854,  0.0793],\n",
      "         ...,\n",
      "         [ 0.1296,  0.0782,  0.1385,  ...,  0.1103,  0.0924,  0.0507],\n",
      "         [ 0.1296,  0.0782,  0.1385,  ...,  0.1103,  0.0924,  0.0507],\n",
      "         [ 0.1296,  0.0782,  0.1385,  ...,  0.1103,  0.0924,  0.0507]]],\n",
      "       grad_fn=<StackBackward0>)))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m d \u001b[38;5;241m=\u001b[39m X_train[:\u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(d\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 20\u001b[0m \u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [33], line 15\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = 768,\n",
    "            hidden_size = 100,\n",
    "            num_layers = 1,\n",
    "            bidirectional = True\n",
    "        )\n",
    "        self.out_layer = nn.Linear(200, 2)\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.lstm(x)\n",
    "        print(x)\n",
    "        return self.out_layer(x)\n",
    "    \n",
    "l = LSTM()\n",
    "d = X_train[:5]\n",
    "print(d.shape)\n",
    "l(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions[:, 1])\n",
    "# plt.xlim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade086c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(Y_test, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(Y_test, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed69d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Y_test, predictions[:, 1] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    model, X_test, Y_test, name=\"Neural Network\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4769a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f5220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
