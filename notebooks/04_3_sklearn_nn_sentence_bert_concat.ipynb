{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae054c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarobyte/envs/malnis/lib/python3.8/site-packages/huggingface_hub/snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from malnis import show\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.sparse as sp\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, auc, roc_auc_score, average_precision_score, PrecisionRecallDisplay\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57351439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jarobyte/malnis_dataset/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8d48d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8965, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>rl</th>\n",
       "      <th>sentences</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>[Our approach is mainly based on the BERT lang...</td>\n",
       "      <td>0.237885</td>\n",
       "      <td>0.065359</td>\n",
       "      <td>0.229075</td>\n",
       "      <td>[KEYWORDS cascade ranking, pre-trained languag...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>[BERT [2] is a self-supervised approach for pr...</td>\n",
       "      <td>0.238372</td>\n",
       "      <td>0.063366</td>\n",
       "      <td>0.215116</td>\n",
       "      <td>[KEYWORDS cascade ranking, pre-trained languag...</td>\n",
       "      <td>[False, False, False, False, False, True, Fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>[Recently, some variants [4, 12] of BERT langu...</td>\n",
       "      <td>0.172727</td>\n",
       "      <td>0.047782</td>\n",
       "      <td>0.172727</td>\n",
       "      <td>[KEYWORDS cascade ranking, pre-trained languag...</td>\n",
       "      <td>[False, False, False, False, False, True, Fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With the capability of modeling bidirectional ...</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>[Recently, some variants [4, 12] of BERT langu...</td>\n",
       "      <td>0.237838</td>\n",
       "      <td>0.078740</td>\n",
       "      <td>0.227027</td>\n",
       "      <td>[KEYWORDS cascade ranking, pre-trained languag...</td>\n",
       "      <td>[False, False, False, False, False, False, Tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural sequence-to-sequence models have provid...</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>[The proposed model is based on the pointer-ge...</td>\n",
       "      <td>0.215139</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.199203</td>\n",
       "      <td>[KEYWORDS cascade ranking, pre-trained languag...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Increasing model size when pretraining natural...</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>[Some others found it beneficial to modify the...</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.083832</td>\n",
       "      <td>0.236287</td>\n",
       "      <td>[KEYWORDS cascade ranking, pre-trained languag...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>[After the new BERT language model is pre-trai...</td>\n",
       "      <td>0.278075</td>\n",
       "      <td>0.100386</td>\n",
       "      <td>0.256684</td>\n",
       "      <td>[KEYWORDS cascade ranking, pre-trained languag...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Recently, the pre-trained language model, BERT...</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>[Different from many other ranking methods whi...</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>0.246032</td>\n",
       "      <td>[KEYWORDS cascade ranking, pre-trained languag...</td>\n",
       "      <td>[True, False, False, False, False, False, Fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sequence-to-Sequence (seq2seq) modeling has ra...</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>[After the query generation model is trained, ...</td>\n",
       "      <td>0.251163</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>[KEYWORDS cascade ranking, pre-trained languag...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Deep Learning Track is a new track for TRE...</td>\n",
       "      <td>KEYWORDS cascade ranking, pre-trained language...</td>\n",
       "      <td>[The details of task construction, evaluation ...</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>[KEYWORDS cascade ranking, pre-trained languag...</td>\n",
       "      <td>[False, False, False, False, False, True, Fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>Proceedings of the 58th Annual Meeting of the ...</td>\n",
       "      <td>[BERT (Devlin et al., 2019) and its improvemen...</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>0.036281</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>[Proceedings of the 58th Annual Meeting of the...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "      <td>Proceedings of the 58th Annual Meeting of the ...</td>\n",
       "      <td>[One was pretrained for WMT 2014 English-Germa...</td>\n",
       "      <td>0.264407</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>0.257627</td>\n",
       "      <td>[Proceedings of the 58th Annual Meeting of the...</td>\n",
       "      <td>[False, False, True, False, False, False, Fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td>Proceedings of the 58th Annual Meeting of the ...</td>\n",
       "      <td>[Gains scale with dataset and model size: RoBE...</td>\n",
       "      <td>0.279245</td>\n",
       "      <td>0.089636</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>[Proceedings of the 58th Annual Meeting of the...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>With the capability of modeling bidirectional ...</td>\n",
       "      <td>Proceedings of the 58th Annual Meeting of the ...</td>\n",
       "      <td>[Other [MASK]less bidirectional models like XL...</td>\n",
       "      <td>0.156977</td>\n",
       "      <td>0.055888</td>\n",
       "      <td>0.145349</td>\n",
       "      <td>[Proceedings of the 58th Annual Meeting of the...</td>\n",
       "      <td>[False, False, True, False, False, False, Fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>Proceedings of the 58th Annual Meeting of the ...</td>\n",
       "      <td>[The two approaches are complementary; for exa...</td>\n",
       "      <td>0.297674</td>\n",
       "      <td>0.113333</td>\n",
       "      <td>0.288372</td>\n",
       "      <td>[Proceedings of the 58th Annual Meeting of the...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Deep Neural Networks (DNNs) are powerful model...</td>\n",
       "      <td>Proceedings of the 58th Annual Meeting of the ...</td>\n",
       "      <td>[with learned functions f, g and a hyperparame...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.083472</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>[Proceedings of the 58th Annual Meeting of the...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Natural language processing tasks, such as que...</td>\n",
       "      <td>Proceedings of the 58th Annual Meeting of the ...</td>\n",
       "      <td>[In Section 3, we show that scores from BERT c...</td>\n",
       "      <td>0.246246</td>\n",
       "      <td>0.095436</td>\n",
       "      <td>0.240240</td>\n",
       "      <td>[Proceedings of the 58th Annual Meeting of the...</td>\n",
       "      <td>[False, False, True, False, False, False, Fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A very simple way to improve the performance o...</td>\n",
       "      <td>Proceedings of the 58th Annual Meeting of the ...</td>\n",
       "      <td>[More generally, one could use any student mod...</td>\n",
       "      <td>0.266212</td>\n",
       "      <td>0.093677</td>\n",
       "      <td>0.238908</td>\n",
       "      <td>[Proceedings of the 58th Annual Meeting of the...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This paper investigates the ability of artific...</td>\n",
       "      <td>Proceedings of the 58th Annual Meeting of the ...</td>\n",
       "      <td>[For acceptability judgments, one finetunes ML...</td>\n",
       "      <td>0.248848</td>\n",
       "      <td>0.094203</td>\n",
       "      <td>0.239631</td>\n",
       "      <td>[Proceedings of the 58th Annual Meeting of the...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Transformers have a potential of learning long...</td>\n",
       "      <td>Proceedings of the 58th Annual Meeting of the ...</td>\n",
       "      <td>[This difference manifests with selfattentive ...</td>\n",
       "      <td>0.194757</td>\n",
       "      <td>0.081301</td>\n",
       "      <td>0.194757</td>\n",
       "      <td>[Proceedings of the 58th Annual Meeting of the...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   We introduce a new language representation mod...   \n",
       "1   The dominant sequence transduction models are ...   \n",
       "2   Language model pretraining has led to signific...   \n",
       "3   With the capability of modeling bidirectional ...   \n",
       "4   Neural sequence-to-sequence models have provid...   \n",
       "5   Increasing model size when pretraining natural...   \n",
       "6   Recently, neural models pretrained on a langua...   \n",
       "7   Recently, the pre-trained language model, BERT...   \n",
       "8   Sequence-to-Sequence (seq2seq) modeling has ra...   \n",
       "9   The Deep Learning Track is a new track for TRE...   \n",
       "10  We introduce a new language representation mod...   \n",
       "11  The dominant sequence transduction models are ...   \n",
       "12  Language model pretraining has led to signific...   \n",
       "13  With the capability of modeling bidirectional ...   \n",
       "14  Recently, neural models pretrained on a langua...   \n",
       "15  Deep Neural Networks (DNNs) are powerful model...   \n",
       "16  Natural language processing tasks, such as que...   \n",
       "17  A very simple way to improve the performance o...   \n",
       "18  This paper investigates the ability of artific...   \n",
       "19  Transformers have a potential of learning long...   \n",
       "\n",
       "                                             document  \\\n",
       "0   KEYWORDS cascade ranking, pre-trained language...   \n",
       "1   KEYWORDS cascade ranking, pre-trained language...   \n",
       "2   KEYWORDS cascade ranking, pre-trained language...   \n",
       "3   KEYWORDS cascade ranking, pre-trained language...   \n",
       "4   KEYWORDS cascade ranking, pre-trained language...   \n",
       "5   KEYWORDS cascade ranking, pre-trained language...   \n",
       "6   KEYWORDS cascade ranking, pre-trained language...   \n",
       "7   KEYWORDS cascade ranking, pre-trained language...   \n",
       "8   KEYWORDS cascade ranking, pre-trained language...   \n",
       "9   KEYWORDS cascade ranking, pre-trained language...   \n",
       "10  Proceedings of the 58th Annual Meeting of the ...   \n",
       "11  Proceedings of the 58th Annual Meeting of the ...   \n",
       "12  Proceedings of the 58th Annual Meeting of the ...   \n",
       "13  Proceedings of the 58th Annual Meeting of the ...   \n",
       "14  Proceedings of the 58th Annual Meeting of the ...   \n",
       "15  Proceedings of the 58th Annual Meeting of the ...   \n",
       "16  Proceedings of the 58th Annual Meeting of the ...   \n",
       "17  Proceedings of the 58th Annual Meeting of the ...   \n",
       "18  Proceedings of the 58th Annual Meeting of the ...   \n",
       "19  Proceedings of the 58th Annual Meeting of the ...   \n",
       "\n",
       "                                              summary        r1        r2  \\\n",
       "0   [Our approach is mainly based on the BERT lang...  0.237885  0.065359   \n",
       "1   [BERT [2] is a self-supervised approach for pr...  0.238372  0.063366   \n",
       "2   [Recently, some variants [4, 12] of BERT langu...  0.172727  0.047782   \n",
       "3   [Recently, some variants [4, 12] of BERT langu...  0.237838  0.078740   \n",
       "4   [The proposed model is based on the pointer-ge...  0.215139  0.093023   \n",
       "5   [Some others found it beneficial to modify the...  0.253165  0.083832   \n",
       "6   [After the new BERT language model is pre-trai...  0.278075  0.100386   \n",
       "7   [Different from many other ranking methods whi...  0.277778  0.090667   \n",
       "8   [After the query generation model is trained, ...  0.251163  0.068493   \n",
       "9   [The details of task construction, evaluation ...  0.392157  0.148936   \n",
       "10  [BERT (Devlin et al., 2019) and its improvemen...  0.164557  0.036281   \n",
       "11  [One was pretrained for WMT 2014 English-Germa...  0.264407  0.086538   \n",
       "12  [Gains scale with dataset and model size: RoBE...  0.279245  0.089636   \n",
       "13  [Other [MASK]less bidirectional models like XL...  0.156977  0.055888   \n",
       "14  [The two approaches are complementary; for exa...  0.297674  0.113333   \n",
       "15  [with learned functions f, g and a hyperparame...  0.200000  0.083472   \n",
       "16  [In Section 3, we show that scores from BERT c...  0.246246  0.095436   \n",
       "17  [More generally, one could use any student mod...  0.266212  0.093677   \n",
       "18  [For acceptability judgments, one finetunes ML...  0.248848  0.094203   \n",
       "19  [This difference manifests with selfattentive ...  0.194757  0.081301   \n",
       "\n",
       "          rl                                          sentences  \\\n",
       "0   0.229075  [KEYWORDS cascade ranking, pre-trained languag...   \n",
       "1   0.215116  [KEYWORDS cascade ranking, pre-trained languag...   \n",
       "2   0.172727  [KEYWORDS cascade ranking, pre-trained languag...   \n",
       "3   0.227027  [KEYWORDS cascade ranking, pre-trained languag...   \n",
       "4   0.199203  [KEYWORDS cascade ranking, pre-trained languag...   \n",
       "5   0.236287  [KEYWORDS cascade ranking, pre-trained languag...   \n",
       "6   0.256684  [KEYWORDS cascade ranking, pre-trained languag...   \n",
       "7   0.246032  [KEYWORDS cascade ranking, pre-trained languag...   \n",
       "8   0.232558  [KEYWORDS cascade ranking, pre-trained languag...   \n",
       "9   0.372549  [KEYWORDS cascade ranking, pre-trained languag...   \n",
       "10  0.164557  [Proceedings of the 58th Annual Meeting of the...   \n",
       "11  0.257627  [Proceedings of the 58th Annual Meeting of the...   \n",
       "12  0.264151  [Proceedings of the 58th Annual Meeting of the...   \n",
       "13  0.145349  [Proceedings of the 58th Annual Meeting of the...   \n",
       "14  0.288372  [Proceedings of the 58th Annual Meeting of the...   \n",
       "15  0.195122  [Proceedings of the 58th Annual Meeting of the...   \n",
       "16  0.240240  [Proceedings of the 58th Annual Meeting of the...   \n",
       "17  0.238908  [Proceedings of the 58th Annual Meeting of the...   \n",
       "18  0.239631  [Proceedings of the 58th Annual Meeting of the...   \n",
       "19  0.194757  [Proceedings of the 58th Annual Meeting of the...   \n",
       "\n",
       "                                            relevance  \n",
       "0   [False, False, False, False, False, False, Fal...  \n",
       "1   [False, False, False, False, False, True, Fals...  \n",
       "2   [False, False, False, False, False, True, Fals...  \n",
       "3   [False, False, False, False, False, False, Tru...  \n",
       "4   [False, False, False, False, False, False, Fal...  \n",
       "5   [False, False, False, False, False, False, Fal...  \n",
       "6   [False, False, False, False, False, False, Fal...  \n",
       "7   [True, False, False, False, False, False, Fals...  \n",
       "8   [False, False, False, False, False, False, Fal...  \n",
       "9   [False, False, False, False, False, True, Fals...  \n",
       "10  [False, False, False, False, False, False, Fal...  \n",
       "11  [False, False, True, False, False, False, Fals...  \n",
       "12  [False, False, False, False, False, False, Fal...  \n",
       "13  [False, False, True, False, False, False, Fals...  \n",
       "14  [False, False, False, False, False, False, Fal...  \n",
       "15  [False, False, False, False, False, False, Fal...  \n",
       "16  [False, False, True, False, False, False, Fals...  \n",
       "17  [False, False, False, False, False, False, Fal...  \n",
       "18  [False, False, False, False, False, False, Fal...  \n",
       "19  [False, False, False, False, False, False, Fal...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"../data/labels.pkl\").reset_index(drop = True)#.head(50)\n",
    "show(data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a3d01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.r2.plot.hist(log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8e3bb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The recently proposed BERT has shown great power on a variety of natural language understanding tasks, such as text classification, reading comprehension, etc. However, how to effectively apply BERT to neural machine translation (NMT) lacks enough exploration. While BERT is more commonly used as fine-tuning instead of contextual embedding for downstream language understanding tasks, in NMT, our preliminary exploration of using BERT as contextual embedding is better than using for fine-tuning. This motivates us to think how to better leverage BERT for NMT along this direction. We propose a new algorithm named BERT-fused model, in which we first use BERT to extract representations for an input sequence, and then the representations are fused with each layer of the encoder and decoder of the NMT model through attention mechanisms. We conduct experiments on supervised (including sentence-level and document-level translations), semi-supervised and unsupervised machine translation, and achieve state-of-the-art results on seven benchmark datasets. Our code is available at \\\\url{https://github.com/bert-nmt/bert-nmt}.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"query\"][36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e05fa16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BERT (Devlin et al., 2019) and its improvements to natural language understanding have spurred a rapid succession of contextual language representations (Yang et al., 2019; Liu et al., 2019; inter alia) which use larger datasets and more involved training schemes.',\n",
       " 'Existing uses of pretrained MLMs in sequenceto-sequence models for automatic speech recognition (ASR) or neural machine translation (NMT) involve integrating their weights (Clinchant et al., 2019) or representations (Zhu et al., 2020) into the encoder and/or decoder during training.',\n",
       " 'We tune the LM weight λ on the development set to minimize word error rate (WER) for ASR or maximize tokenized BLEU for NMT.',\n",
       " 'We finetune BERT to produce scores without [MASK] tokens.',\n",
       " 'In particular, we see the outsized cost of the unconditional first unigram in Figure 3.',\n",
       " 'Furthermore, neither work considers the inference cost of masked rescoring, which we address with our maskless scoring approach, or analyze PLL’s numerical properties.',\n",
       " 'Their PLLs would have different properties from ours (e.g., their cross-entropies in Figure 4 may be convex instead of flat).',\n",
       " 'We studied scoring with MLM pseudo-loglikelihood scores in a variety of settings.',\n",
       " 'Thus, one could compute a new pretrained model’s word-normalized PPPL on a small target- domain sample to quickly assess whether rescoring with it could improve on the previous model.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s, l in zip(data.sentences[36], data.relevance[36]) if l == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f45f2977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.relevance[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d57f0b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer(\n",
    "    \"sbert\"\n",
    "#     'all-MiniLM-L6-v2', \n",
    "#     cache_folder = \"../assets\"\n",
    "#     \"../cache/huggingface/transformers/\"\n",
    "#     cache_folder = \"../cache/huggingface/transformers\"\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ff1df69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4979e21e08f84d55bd4bd909045dd64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = model.encode(data[\"query\"], show_progress_bar = True)\n",
    "query_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = query_embeddings.shape[1]\n",
    "sentence_embeddings = [\n",
    "    model.encode(l)#.toarray() \n",
    "    for l in tqdm(data.sentences)\n",
    "]\n",
    "all([l.shape[1] == dims for l in sentence_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff9f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_embeddings.shape[0] == len(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(l) for l in tqdm(data.sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\n",
    "#     ((sp.csr_matrix(np.ones([l.shape[0],1])) * q) - l).power(2)\n",
    "#     (q - l)**2\n",
    "    np.concatenate([np.tile(q, (l.shape[0], 1)), l], axis = 1)\n",
    "    for q, l in zip(tqdm(query_embeddings), sentence_embeddings)\n",
    "]\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate(train)#.toarray().T#.squeeze()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d616105",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_sbert_concat.npy\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65ed2afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222254, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load(\"X_sbert_concat.npy\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "632ff8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95f9ee1966a4afaad0972ce3434e493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8965 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3173352,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([y for l in tqdm(data.relevance) for y in l])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "568c0951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97353"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9a816ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030678285926049174"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e15b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = LogisticRegression(\n",
    "#     C = 10**-2\n",
    "# )\n",
    "# model.fit(X, Y)\n",
    "\n",
    "# predictions = model.predict_proba(X)\n",
    "\n",
    "# # predictions.sum()\n",
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = LogisticRegression(\n",
    "#     C = 10**-2\n",
    "# )\n",
    "# model.fit(X, Y)\n",
    "\n",
    "# predictions = model.predict_proba(X)\n",
    "\n",
    "# # predictions.sum()\n",
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50be69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = SVC(\n",
    "# #     C = 10**-2\n",
    "# )\n",
    "# model.fit(X, Y)\n",
    "\n",
    "# predictions = model.predict_proba(X)\n",
    "\n",
    "# # predictions.sum()\n",
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0426c05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = MLPClassifier(\n",
    "# #     C = 10**-2\n",
    "#     verbose = True,\n",
    "# #     early_stopping = True\n",
    "# )\n",
    "# model.fit(X, Y)\n",
    "\n",
    "# predictions = model.predict_proba(X)\n",
    "\n",
    "# # predictions.sum()\n",
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35f8b580",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [222254, 3173352]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_test\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/sklearn/model_selection/_split.py:2445\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2445\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2447\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2448\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2449\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2450\u001b[0m )\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/sklearn/utils/validation.py:433\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 433\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/sklearn/utils/validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    385\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    390\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [222254, 3173352]"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_test\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad173cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.sum(), Y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb2183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = MLPClassifier(\n",
    "#     C = 10**-2\n",
    "    verbose = True,\n",
    "#     early_stopping = True\n",
    ")\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "predictions = model.predict_proba(X_test)\n",
    "\n",
    "# predictions.sum()\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions[:, 1])\n",
    "# plt.xlim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade086c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(Y_test, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(Y_test, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed69d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Y_test, predictions[:, 1] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf60c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    model, X_test, Y_test, name=\"Neural Network\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4769a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f5220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
